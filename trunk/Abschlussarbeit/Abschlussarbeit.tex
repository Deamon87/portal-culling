\documentclass[12pt]{scrreprt}
\parindent 0pt
\parskip 12pt

\usepackage[latin1]{inputenc}
\usepackage[ngerman]{babel}
\usepackage[colorlinks=false,pdfborder={0 0 0}]{hyperref}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{color}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{scrhack}
\DeclareGraphicsExtensions{.eps}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{darkred}{rgb}{0.5,0,0}

\begin{document}
\lstset{language=Java,
		basicstyle=\ttfamily,
		numbers=left,
		frame=single,
		commentstyle=\color{darkgreen},
		keywordstyle=\color{darkred},
		stringstyle=\color{blue}}

\pagenumbering{Roman}
\tableofcontents

\chapter{Einleitung}
\pagenumbering{arabic}

In der heutigen Zeit haben sich viele elektronische Unterhaltungsmedien
entwickelt und sind im Alltag nicht mehr wegzudenken. Ein besonders großer
Industriezweig, der sich in den letzten Jahren rasant entwickelt hat, ist die
Videospielindustrie.

In kürzester Zeit haben sich aus pixelbasierten 2D-Grafiken komplexe 3D-Welten
entwickelt, die kaum von realen Bilder zu unterscheiden sind. Möglich wird all
dies durch immer leistungsfähigere Hardware, die von Jahr zu Jahr immer mehr
Performance im Bereich der 3D-Echtzeitberechnung bringt.

Um die Hardware auch für die Entwickler programmierbar zu machen, stehen mehrere
Grafik APIs\footnote{Application Programming Interface} zur Verfügung. Die
beiden bekanntesten sind Microsofts Direct3D und die freie Grafikbibliothek
OpenGL. OpenGL ist ein wichtiger Bestandteil dieser Arbeit und wird im späteren
Verlauf der Arbeit näher betrachtet.

Mit Hilfe dieser Grafikbibliotheken lassen sich, mit relativ wenig Aufwand
Anwendungen mit 3D-Inhalten programmieren. Da die APIs der Bibliotheken in
viele Befehle besitzen, die low-level Grafikbefehle unterstützen steigt der
Aufwand bei größeren Projekten erheblich. Heutzutage werden 3D-Anwendungen, wie
zum Beispiel Spiele nicht von Grund auf neu entwickelt. In diesem Fall werden 
Grafik-Engines eingesetzt, die eine höhere Abstraktionsschicht besitzen und
somit effektiver programmiert werden kann.

Grafik-Engines unterstützen von vornherein wichtige Konzepte, die bei der
heutigen Spieleentwicklung angewendet werden. Eines dieser Konzepte ist die
Shaderunterstütz-ung, mit der sich zum Beispiel Oberflächeneigenschaften
simulieren oder auch optische Effekte, wie Wasserspiegelungen erzeugen lassen. 
Moderne Engines bieten zudem Komponenten wie Kameras, Szenegraphen und
Multithreaded-Rendering. Die verbreitetsten Engines sind die Unreal Engine von
Epic Games, die Source Engine von Valve Corporation und die Cry-Engine von
Crytek.

Im Mittelpunkt dieser Arbeit steht das jVR-Framework. Es stellt eine 3D-Engine
dar, die komplett auf Java aufgebaut ist. Als Grafikbibliothek wird OpenGL 
verwendet. Um OpenGL in Java nutzen zu können, wird die Wrapper-Bibliothek
JOGL verwendet. Ein Besonderheit des jVR-Frameworks ist die Möglichkeit, Virtual
Reality Systeme zu entwerfen. Dies bedeutet es können 3D-Szenen erzeugt werden
die einen räumlichen Effekt haben. Die Beuth Hochschule für Technik besitzt ein
Holodeck, wo mehrere Anwendungen demonstriert werden, die mit jVR entwickelt
wurden.

Ein weiteres Konzept in der Spieleentwicklung ist das sogenannte Portal Culling
bzw. Portal Rendering. Dieses Konzept beschreibt eine Szene durch ein
Portal. Das bedeutet, man kann durch ein Portal treten und befindet
sich im Nachhinein an einem Ort der vorher mehrere 100m weit weg war. Besonders
populär wurde diese Art des Renderings mit der Veröffent-lichung des Spiels
Portal von Valve Corporation. Darin wird der Spieler in die Lage versetzt Portal
von Hand zu erschaffen und so Orte zu erreichen, die zuvor unerreichbar schienen.

Um Entwicklern die Möglichkeit zu bieten, selbst ein Spiel im Stile von Portal
entwickeln zu können, besteht der Schwerpunkt dieser Arbeit darin das 
jVR-Framework um die Funktion zu erweitern, Portale nach Belieben zu erschaffen
und zu platzieren. Portale können dann wie Kameras oder Objekte in Szenen
verwendet werden.

\chapter{Aufgabenstellung}

Die Aufgabe besteht darin eine Erweiterung in das jVR-Framework in Form einer
Portal-Funktion zu integrieren. Man wird dadurch in die Lage versetzt innerhalb
eines vordefinierten Raumes durch verschiedene Portale zu treten und bestimmte
Orte innerhalb des Raumes zu erreichen.

Weiterhin wird man in die Lage versetzt Portale bei Bedarf selbst zu erschaffen
und zu positionieren. Außerdem kann man bestimmte Objekte nehmen und durch die
Räume bewegen. Dabei sollen sich diese Objekte physikalisch korrekt verhalten
und mit in die Portal-Funktionalität integriert werden. Das soll heißen, falls
ein Objekt durch ein Portal befördert wird, soll es aus dem entsprechenden
Gegenstück auch entsprechend der Erwartung physikalisch korrekt austreten.

Als Ansatz dient das Portal-Rendering-Verfahren, welches in der Computergrafik
dafür benutzt wird geschlossene Räume, die in verschiedene Segmente aufgeteilt
sind untereinander zu verbinden und je nach Sichtbarkeit zu rendern. Dabei
werden Lichteinfall und Schatteneinfall, sowie die Position der Kamera
berücksichtigt.

Durch die Erweiterung des jVR-Frameworks, welches speziell mit dem Gedanken
implementiert wurde um das Holodeck der BHT und stereoskopische
Rendering-Verfahren zu benutzen um 3dimensionale Szenen erzeugen die mit einer
3D-Brille betrachtet werden. Weitere Möglichkeiten bietet das
Headtracking-Verfahren um die Gesichtsposition zu ermitteln und so die
Perspektive dementsprechend anzupassen.

Das Ergebnis der Aufgabe wird eine Beispielimplementierung sein, die zum Einen
die Erschaffung von Portalen demonstriert. Des Weiteren werden verschiedene
Beispielszenen erstellt, in denen das Portal-Rendering und Lösungen
verschiedener Problemfälle, die bei dem vorgestellten Rendering-Verfahren
auftreten können vorgestellt werden.

\section{Motivation}

Im Rahmen des Masterkurses \emph{Computer Graphics and Effects} an der Beuth
Hochschule für Technik wird den Teilnehmern ein Verständnis für
3D-Echtzeitprogrammierung vermittelt. Der Kurs befasst sich vor allem mit dem
Einsatz von Shadern und deren Programmierung. Im Laufe des Kurses wird eine
3D-Echtzeitanwendung entwickelt. Zur Erstellung der Anwendung wird das
jVR-Framework verwendet.

Das Framework besitzt bereits eine Vielzahl an Möglichkeiten 3D-Anwendungen zu
erstellen. Es können Shader eingesetzt, Modelle geladen und transformiert und
Schatten erzeugt werden.

Im Falle dieser Arbeit, wird ein besonderer Aspekt in der 3D-Echtzeitberechnung
beleuchtet , das Portal Rendering bzw. Portal Culling. Innerhalb der Arbeit
werden die verschiedenen Anwendungsmöglichkeiten dieser Technik dargelegt und
mit Beispielen fundiert.

Als Basis dieser Arbeit dient das jVR-Framework, welches um die Funktionen des
Portal Cullings erweitert wird. Dies ermöglicht den Aufbau von Szenen, die sich
in weitere Szenen unterteilen lassen.

\section{Aufbau der Arbeit}

Zu Beginn der Arbeit werden mehrere Konzepte in Bezug auf Portal Rendering bzw.
Portal Culling vorgestellt. Dies soll einen Überblick geben in welchen Rahmen
diese Technik eingesetzt wird und welchen Nutzen in 3D-Anwendungen daraus
gezogen werden können. Zusätzlich werden Beispiele aufgezählt in denen sich die
Anwendung von Portal Rendering besonders lohnt.

Ein weiterer Teil der Arbeit betrifft das jVR-Framework. In diesem Abschnitt
werden die verschiedenen Konzepte, die in diesem Framework ihre Anwendung finden
genauer beleuchtet. Zudem wird darauf eingegangen wie Anwendungen im
jVR-Framework umgesetzt werden. Die Standards die in dem Framework verwendet
werden, werden ebenfalls behandelt.

Der nächste Abschnitt befasst sich mit dem Entwurf des Portal Cullings. Hier
werden die verschiedenen Portaltypen vorgestellt, die in der Implementierung
erstellt werden sollen. Es werden außerdem die verschiedenen Vorgehensweisen
betrachtet, mit denen die Portale erstellt werden.

Das Kapitel der Praktischen Umsetzung befasst sich mit der eigentlichen
Implementierung. Dort werden Klassen und Funktionalitäten näher erklärt. Es
werden die verschiedenen Techniken betrachtet mit denen die Portale realisiert
wurden. Außerdem werden die verwendeten DesignPattern begründet und
dargelegt.

Das vorletzte Kapitel gibt einen Überblick über die Ergebnisse. Es
wird untersucht, in wie fern die Anforderungen der Arbeit erfüllt wurden. Zudem
werden die Ergebnisse kritisch betrachtet und analysiert.

Im letzten Kapitel wird eine Zusammenfassung gegeben, die nocheinmal die Arbeit
in kurzen Worten wiedergibt. Zuletzt wird ein Ausblick gemacht, wie Ergebnisse
verbessert bzw. erweitert werden können.

\chapter{Stand der Technik}

Das folgende Kapitel soll einen Überblick darüber geben, was Portal Culling ist
und wie es in der 3D-Echtzeitgrafik Anwendung findet. Zudem werden andere
Algorithmen vorgestellt, die die Performanz in 3D-Anwendungen steigern. Im
weiteren Verlauf wird das jVR-Framework genauer betrachtet und wichtige
Funktionen diskutiert. Die Standards, die in dem Framework verwendet werden,
werden ebenfalls vorgestellt.

\section{Portal Culling}

Portal Culling ist ein Verfahren, um die Leistung in 3D-Echtzeitberechnungen zu
steigern. In diesem Verfahren werden bestimmte Objekte ausgewählt, die
letztendlich gerendert werden und damit für den Betrachter sichtbar werden.

Beim Portal Culling stellt man sich einen Bereich vor, der aus mehreren Zellen
besteht. Eine Zelle setzt sich aus mehreren Wänden zusammen und kann an weitere
Zellen angrenzen. Diese Zellen verbinden einander durch Türen bzw. Fenster, so
genannte Portale. Portale sind in der Regel durchsichtig und können in den
meisten Fällen auch betreten werden.

Dadurch lassen sich große Szenen erstellen, die innerhalb eines Gebäudes oder
eines Tunnelsystems spielen. Die Komplexität lässt sich beliebig skalieren, da
sich beliebig viele Zellen miteinander verbinden lassen.

Der Sinn des Portal Cullings besteht darin, Objekte innerhalb der
verschiedenen Zellen zu rendern bzw. nicht zu rendern je nach Sichtbarkeit des
jeweiligen Objektes. Wird z.B. ein Objekt durch eine Wand verdeckt, besteht
keine Notwendigkeit das Objekt zu rendern und es wird zudem Rechenzeit
gespart. Durch den Einsatz dieses Verfahrens können bis zu 50 Prozent der
benötigten Ressourcen zum Rendern eingespart werden. %TODO: belegen

% \begin{figure}[h]
% \begin{center}
% \includegraphics[height=50mm]{img/portalsplate1}
% \end{center}
% \caption{Test 1234}
% \end{figure}

Zellen können auch ganz vom Renderprozess ausgeschlossen werden. Dies geschieht,
wenn das Portal, das zur nächsten Zelle führt außerhalb des Sichtbereiches
liegt. Dadurch wird die komplette Zelle und alle enthaltenen Objekte nicht
gerendert.

Der erste Portal Culling Algorithmus wurde 1990 von Airey vorgestellt. Im
späteren Verlauf wurden von \cite{visComp} bzw. \cite{visAlgo}
verbesserte komplexere und vor allem effizientere Algorithmen zum Thema Portal
Culling entwickelt.

All diese Algorithmen gleichen sich in der Annahme, dass die Wände als
verdeckendes Element für Szenen dienen, die innerhalb eines Raumes stattfinden.
Des Weiteren wird durch jedes Portal ein View Frustum Culling (siehe Frustum
Culling) durchgeführt. Das eigentliche Frustum wird durch das Portal auf dessen
Größe reduziert, somit werden alle Objekte und auch Portale außerhalb des
Sichtbereichs vom Rendern ausgeschlossen. Diese Vorgehensweise kann rekursiv
fortgesetzt werden, falls sich innerhalb des betrachteten Raumes ein weiteres
Portal befindet.

Um Portal Culling zu betreiben muss ein gewisses Maß von Vorberechnungen
gemacht werden. Nach \cite{portals} muss eine Szene in folgenden Schritten
gerendert werden um ein optimales Ergebnis zu erreichen.

\begin{enumerate}
  \item Als erstes muss die Zelle \emph{V} bestimmt werden, in der sich der
  Betrachter bzw. die Betrachterkamera befindet.
  \item Des Weiteren soll eine 2-dimensionale \emph{Bounding Box} \emph{P}
  erzeugt werden, die in ihren Maßen dem rechteckigen Bildschirm entspricht.
  \item Danach wird die Geometrie der Zelle \emph{V} gerendert. Dabei wird das
  Verfahren des View Frustum Culling angewendet. Das Frustum\footnote{Körper
  dessen Inhalt auf den Bildschirm projeziert wird}, das vom Betrachter ausgeht
  wird aus dem Rechteck \emph{P} gebildet, in diesem Falle ist das der ganze Bildschirm.
  \item Alle Portale der benachbarten Zellen von \emph{V} werden rekursiv
  durchlaufen. Für jedes Portal der aktuellen Zelle, in der sich der Betrachter
  befindet wird das Portal auf den Bildschirm projeziert. Daraufhin wird eine
  2-dimensionale, an den Achsen ausgerichtete \emph{Bounding Box} (BB)
  erzeugt. Dann wird die Schnittmenge zwischen der BB und \emph{P} ermittelt.
  \item Für jede Schnittmenge gilt: Ist die Schnittmenge leer, ist die
  benachbarte Zelle, die über das Portal verbunden ist, aus dem aktuellen
  Betrachterstandpunkt nicht sichtbar. Daher kann die Zelle aus dem
  Rendervorgang ausgeschlossen werden. Ist die Schnittmenge wiederum nicht leer,
  wird der Inhalt der Zelle an Hand des Frustums, welches vom Betrachter
  ausgeht, und aus der Schnittmenge gecullt.
  \item War die Schnittmenge nicht leer, kann es möglich sein, dass die Nachbarn
  der benachbarten Zelle ebenfalls sichtbar für den Betrachter sind. Dabei wird
  der dritte Schritt wiederholt. Diesmal ist die Schnittmenge \emph{P}.
  Jedes Objekt, welches gerendert wurde, sollte markiert werden. Das hat den
  Grund es kein 2tes Mal rendern zu müssen.
\end{enumerate}

\subsection{Portale als Spiegel}

Ein weiterer Verwendungszweck von Portal Culling ist die Erzeugung von Spiegeln.
Spiegel reflektieren den Sichtbereich des Betrachters. Wenn man aus Sicht des
Spiegels in die Szene schaut und diese nochmals spiegelt, erhält man das Bild
welches der Betrachter bei Blick in den Spiegel erhält. 

Man nimmt den Spiegel in diesem Fall als Spiegelebene und erstellt an der
gespiegelten Position des Betrachters eine Kamera. Diese rendert dann die Szene
durch den Spiegel. Danach wird die gerenderte Szene aus Sicht der Spiegelkamera
auf den Spiegel projeziert.

Spiegel werden häufig in Szenen eingesetzt, die eingerichtete Wohnungen oder
andere Räume innerhalb eines Hauses repräsentieren sollen. Dadurch bekommt der
Betrachter einen Eindruck davon, wie der Spiegel innerhalb des Raumes wirkt.

In Computerspielen dienen Spiegel oft als Stilmittel, um z.B. die eigene
Spielfigur betrachten zu können (z.b. in First-Person Shootern). Oder um
atmosphärische Spannung zu erzeugen, wenn z.B. eine geistähnliche Gestalt im
Spiegel erscheint.

Mit Hilfe von Shadern kann hinzukommend noch eine glasähnliche Oberfläche
simuliert werden, was den Eindruck eines Spiegels nochmals verstärkt. Außerdem
lassen sich zudem noch Materialschäden oder Abnutzungserscheinungen hinzufügen
um den Eindruck von Verschleiß zu erwecken.

\subsection{Portale als Teleporter}

Portale lassen sich vor allem dazu nutzen, sich innerhalb der Umgebung zu
teleportieren. Das bedeutet, dass man sich von einem Ort zu einem anderen
Ort in kürzester Zeit bewegt, obwohl diese sehr weit von einander entfernt
liegen. Dazu werden 2 oder auch nur ein Portal erzeugt und der Ausgang des einen
Portals stellt das Frustum des anderen Portals dar. 

Dabei wird eine Kamera benutzt die genau den entgegengesetzten Sichtbereich des
ersten Portals rendert, wobei aber der Sichtbereich auf das zweite Portal
gerendert wird. Bei Durchtreten der beiden Portale wird die Position des
Betrachters dementsprechend verändert.

Möglich ist es zudem ein Portal als Einbahnstraße zu benutzen, wodurch der
Betrachter nicht mehr in der Lage ist an die Stelle des durchtretenden Portals
ohne Probleme zurückzukehren. Dies bedeutet, dass der Betrachter zwar von
Stelle 1 nach Stelle 2 teleportiert wird, wenn er das Portal durchtritt, aber
nicht mehr von Stelle 2 an die Stelle 1 gelangen kann, da sich dort kein Portal
befindet. 

Man erreicht dies, indem man die Szene an der Stelle durch das Portal rendert,
an der der Spieler austreten soll. Daher wird an diese Stelle mit dem Verhältnis
zur Betrachterkamera die Szene gerendert und auf der Portaloberfläche gerendert.
Diese Herangehensweise lässt sich oft in Computerspielen wiederfinden, um z.B.
ein Labyrinth zu erzeugen oder den Spieler kurz zu desorientieren.

\subsection{Frustum Culling}

Frustum Culling ist ein Verfahren in der Computergrafik um die Performanz beim
Rendern von Szenen zu steigern. Bei diesem Verfahren werden für den Betrachter
unsichtbare Objekte vom Renderprozess ausgeschlossen.

Beim Frustum Culling wird ein Sichtbereich aufgespannt, das so genannte Frustum.
Es wird überprüft welche Objekte sich innerhalb des Frustums befinden und welche
nicht. Befindet sich ein Objekt innerhalb, wird es gerendert. Ist dies nicht der
Fall, wird es verworfen.

Es werden geometrische Primitive um ein Objekt gezeichnet, um festzustellen, ob
es sich innerhalb des Frustums befindet. Diese Primitive nennt man \emph{Bounding
Boxes} (BB). Da die BB größer ist als das eigentliche Objekt, kann es dazu
führen, dass ein Objekt zwar nicht mehr zu sehen ist aber noch mit seiner BB den
Frustum berührt und daher noch gerendert wird.

\subsection{Occlusion Culling}

Ein weiteres Verfahren um die Performanz zu optimieren ist das Occlusion
Culling. Ähnlich dem Frustum Culling werden Objekte bestimmt, die nicht in den
Renderprozess gehören, da nicht sichtbar.

Anders als beim Frustum Culling werden Objekte nicht außerhalb des Frustums
verworfen. Beim Occlusion Culling wird bestimmt, ob ein Objekt von einem anderen
Objekt verdeckt und damit für den Betrachter nicht sichtbar ist. Diese
verdeckenden Objekte werden als Occluder bezeichnet.

Es gibt 2 Ansätze Occlusion Culling durchzuführen. Einmal gibt es den
punktbasierten Ansatz. In diesem wird die Szene aus Sicht des Betrachters bzw.
eines festen Punktes betrachtet. Dabei werden alle Objekte betrachtet und je
nach Sichtbarkeit zum Renderprozess hinzugefügt oder ausgeschlossen. Diese
Vorgehensweise muss bei Bewegung des Punktes wiederholt werden.

Der zweite Methode stellt der zellenbasierte Ansatz dar. Bei dieser
Vorgehensweise wird eine Zelle von einer bestimmten Größe definiert. Danach
werden alle Objekte bestimmt die innerhalb dieser Zelle für den Betrachter nicht
sichtbar sind, da verdeckt. Diese Methode bietet den Vorteil den Vorgang nur
dann zu wiederholen, falls man den Bereich der Zelle verlässt.

\section{Techniken des jVR-Framework}
% TODO: besser formulieren, jVR wurde nicht von mir ausgewählt, ist Teil der
% Arbeit
Der Einsatz des jVR-Frameworks spielt eine zentrale Rolle dieser Arbeit. In
diesem Teil der Arbeit werden die wichtigsten Konzepte vorgestellt, die in dem
Framework verwendet werden.

Das jVR-Framework ist im Rahmen einer Masterarbeit entstanden. Das Framework
soll den Studenten eine Plattform liefern mit denen sie komplexe
3D-Programme erzeugen können. Außerdem soll es den Studenten ein besseres
Verständnis über die Funktionen von 3D-Engines vermitteln.

\subsection{Shaderunterstützung}

%TODO: Shaderunterstützung
Ein Grund für die Entwicklung des jVR-Frameworks war es den Studierenden der BHT
Berlin eine Umgebung zu liefern, mit der sich problemlos Shaderbasierte
Anwendungen entwickeln lassen. Shader werden im Abschnitt \emph{GLSL} näher
betrachtet. Die zuvor verfügbaren Engines boten zwar eine Unterstützung für
Shader an, waren aber nicht in Java entwickelt.

Da der Schwerpunkt im Studium an der BHT auf der Programmiersprache Java liegt,
sollte die Engine mittels Java programmiert werden können. Somit wurde das
jVR-Framework mit dem Gedanken entwickelt, Shader schreiben und verwenden zu
können.

Shader können im jVR-Framework aus 3 verschiedenen Typen bestehen. Der
Fragment-Shader, der Vertex-Shader und der Geometry-Shader. Nötig für eine
funktionierende Shaderanwendung sind aber nur der Fragement- und Vertex-Shader.

Im jVR-Framework besitzt jedes geometrische Objekt eine \texttt{ShapeNode}.
Dieser \texttt{ShapeNode} kann ein Material zugeordnet werden. Eines dieser
Materialien ist das \texttt{ShaderMaterial}. Einem \texttt{ShaderMaterial}
können mehrere Shaderkontexte zugeordnet werden.

Die Shaderkontexte beinhaltet ein Shaderprogramm, zum Beispiel einen
Fragement- sowie einen Vertex-Shader. Die Shaderprogramme können mittels
Uniformvariablen angesprochen und in ihrem Verhalten verändert werden.

\subsection{Szenegraph}

Um die Objekte innerhalb einer 2D bzw. 3D-Szene verwalten zu können, wird in dem
jVR-Framework Verwendung von einem Szenegraphen gemacht. Ein Szenegraph
ist eine baumförmige Datenstruktur. Objekte in einer Szene können zum einen
geometrische Objekte oder Lichtquellen sein. Jedes Objekt in der Szene besitzt
eine eigene Transformation. Es besteht außerdem die Möglichkeit Objekte einer
Szene zu animieren. Dies geschieht, indem man die Transformationen in zeitlichen
Intervallen manipuliert.

Über einen Szenengraph lassen sich außerdem Objekte animieren, die in
hierarchischer Beziehung zueinander stehen. Werden mehrere Objekte in einem
Gruppenknoten zusammengefasst entsteht eine hierarchische Beziehung. Dabei kann
der ganze Gruppenknoten transformiert werden, mitsamt den enthaltenen Objekten
oder aber jeder einzelne Objektknoten innerhalb des Gruppenknotens.

\subsubsection{Szeneknoten}

Das jVR-Framework unterscheidet zwischen mehreren Knotentypen. Die
Gemeinsamkeit aller Knoten ist, dass sie von der abstrakten Oberklasse
\texttt{SceneNode} abgeleitet werden. 

Es gibt zum einen den schon erwähnten \texttt{GroupNode}. Dieser Knoten
kann als einziger Kinderknoten enthalten. Das bedeutet, der \texttt{GroupNode}
kann zum einen Blattknoten, zum Beispiel ein \texttt{ShapeNode} oder aber
einen weiteren \texttt{GroupNode} enthalten. Dadurch entsteht eine hierarchische
Ordnung zwischen den Knoten

Zusätzlich gibt es den \texttt{LightNode}. Dieser repräsentiert die
verschiedenen Lichtquellen und wird von 3 Standardlichtquellen abgeleitet. Diese
3 Standardlichtquellen sind die \texttt{PointLightNode},
die \texttt{DirectionalLightNode} und die \texttt{SpotLightNode}.

Die \texttt{PointLightNode} strahlt Licht in alle Richtungen gleichzeitig. Sie
eignet sich vor allem dazu geschlossene Räume auszuleuchten. Anders hingegen die
\texttt{SpotLightNode}, die Licht nur innerhalb eines gerichteten Kegels
ausstrahlt. Mit ihr lassen sich zum Beispiel Taschenlampen oder
Straßenlaternen simulieren. Die letzte Lichtquelle stellt die
\texttt{DirectionalLightNode} dar. Sie strahlt Licht parallel in eine bestimmte
Richtung. Mit dieser Lichtquelle lässt sich gut das Sonnenlicht abbilden.

Damit eine Szene betrachtet und auf den Bildschirm projeziert werden kann wird
eine Kamera benötigt. Der passende Knotentyp ist die \texttt{CameraNode}. Hinzu
kommt die \texttt{VRCameraNode}, die für den Einsatz in VR-Systemen gedacht ist.

Um geometrische Objekte in einer Szene abbilden zu können, wird die
\texttt{ShapeNode} genutzt. Einer \texttt{ShapeNode} lassen sich eine Geometrie
und ein passendes Material zuordnen. \texttt{ShapeNodes} lassen sich im
Szenegraphen wiederverwenden. Dazu wird die \texttt{ShapeNode} an verschiedene
\texttt{GroupNodes} gehängt, die unterschiedlich transformiert werden. 

Als Beispiel sei ein Tisch mit 4 Beinen. Man benötigt nur die Tischplatte und
ein Tischbein in Form einer \texttt{ShapeNode}. Das Tischbein wird in 4
verschiedene \texttt{GroupNodes} gepackt und mittels Transformation unter der
Tischplatte angeordnet.

Der letzte Knotentyp, über den das jVR-Framework verfügt, ist die
\texttt{ClipPlaneNode}. Mit ihr lassen sich Vertices komplett ausblenden. Wenn
eine \texttt{ClipPlaneNode} auf die Szene gepackt wird, wird die Szene in 2
Halbräume unterteilt. Die Vertices die sich auf der Rückseite der Ebene befinden
werden ausgeblendet.

\subsubsection{Transformationen}

Das jVR-Framework verfügt über mehrere Möglichkeiten Knoten innerhalb einer
Szene zu transformieren. Es werden 5 Arten der Transformation unterschieden,
vergleiche dazu auch \cite{compOL}.

\begin{align}
\textbf{Translation:} &
\begin{pmatrix}
\phantom{-\sin \alpha}&\phantom{-\sin \alpha}&\phantom{-\sin
\alpha}&\phantom{-\sin \alpha}\\[-2.5ex] 
1 & 0 & 0 & {t}_{x} \\
0 & 1 & 0 & {t}_{y} \\
0 & 0 & 1 & {t}_{z} \\
0 & 0 & 0 & 1
\end{pmatrix} \\
\textbf{Skalierung:} &
\begin{pmatrix}
\phantom{-\sin \alpha}&\phantom{-\sin \alpha}&\phantom{-\sin
\alpha}&\phantom{-\sin \alpha}\\[-2.5ex]
{s}_{x} & 0 & 0 & 0 \\
0 & {s}_{y} & 0 & 0 \\
0 & 0 & {s}_{z} & 0 \\
0 & 0 & 0 & 0
\end{pmatrix}
\end{align}
\begin{align}
\textbf{Rotation um X-Achse:} &
\begin{pmatrix}
\phantom{-\sin \alpha}&\phantom{-\sin \alpha}&\phantom{-\sin
\alpha}&\phantom{-\sin \alpha}\\[-2.5ex] 
1 & 0 & 0 & 0 \\
0 & \cos \alpha & -\sin \alpha & 0 \\
0 & \sin \alpha & \cos \alpha & 0 \\
0 & 0 & 0 & 1 
\end{pmatrix} \\
\textbf{Rotation um Y-Achse:} &
\begin{pmatrix}
\phantom{-\sin \alpha}&\phantom{-\sin \alpha}&\phantom{-\sin
\alpha}&\phantom{-\sin \alpha}\\[-2.5ex] 
\cos \beta & 0 & \sin \beta & 0 \\
0 & 1 & 0 & 0 \\
-\sin \beta & 0 & \cos \beta & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}\\
\textbf{Rotation um Z-Achse:} &
\begin{pmatrix}
\phantom{-\sin \alpha}&\phantom{-\sin \alpha}&\phantom{-\sin
\alpha}&\phantom{-\sin \alpha}\\[-2.5ex] 
\cos \gamma & -\sin \gamma & 0 & 0 \\
\sin \gamma & \cos \gamma & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}
\end{align}

\subsubsection{Traversierung}

Um eine baumförmige Datenstruktur wie den Szenegraph zu durchlaufen, wird
Gebrauch von der Traversierung gemacht. Dabei werden die einzelnen Knoten der
Szene verarbeitet. Traversierung macht vor allem dann Sinn, wenn z.B.
Szeneknoten mit bestimmten Eigenschaften gesucht werden müssen. 

Zum Rendern muss der Szenegraph ebenfalls durchlaufen werden. Angefangen wird
dabei beim Wurzelknoten. Jeder Szeneknoten der an der Wurzel hängt wird in die 
Welttransformation überführt. Dies geschieht, indem alle Transformationen vom
Wurzelknoten bis zum Szeneknoten multipliziert werden.

\subsection{Drawlisten}

In Drawlisten werden die verschiedenen Szeneknoten mit ihren Weltransformationen
gesammelt und nacheinander gerendert. Eine Drawliste kann zudem beliebig oft
verwendet werden. Das bietet den Vorteil, den Szenegraph nicht nochmals zu
traversieren, falls ein Objekt mehrmals gerendert werden muss.

Nachdem die Drawlisten erstellt wurden, werden sie in der Pipeline gespeichert
und können je nach Bedarf genutzt werden. Das heißt, dass Objekte nach
bestimmten Materialeigenschaften gefiltert und gerendert werden können, siehe
\cite[S. 47-48]{jvrMast}.

\subsection{Pipelinekonzept}

Die Pipeline im jVR-Framework macht es möglich einzelne Szeneobjekte getrennt
voneinander zu rendern. Man kann zudem verschiedene Materialien zu
Materialklassen zusammenfassen. Materialklassen können wiederum aus mehreren
Shaderkontexten bestehen und somit aus mehreren Shaderprogrammen. 

Shaderkontexte werden größtenteils dafür verwendet Objekte richtig zu
beleuchten. Erst wird mit einem Shaderprogramm die Umgebungsbeleuchtung
(Ambient-Pass) für das Objekt gerendert. Anschließend darauf werden die Objekte
für jede Lichtquelle mit dem Shaderprogramm mit der eigentlichen Beleuchtung
gerendert (Lighting-Pass). Die zuvor erzeugten Bildinformationen werden mittels
des Blending-Verfahrens aufaddiert, (vergleiche auch \cite[S. 46]{jvrMast}).

\subsection{FrameBuffer Object}

Um beim jVR-Framework mehrere Szenen in einem Bild zusammenlegen zu können, wird
das \emph{FrameBuffer Object}, kurz FBO verwendet. Mit dem \emph{FrameBuffer
Object} kann direkt in eine Textur gerendert werden. Das bietet den Vorteil,
dass eine Szene erst erzeugt wird und in die Textur geschrieben wird. Danach
kann die erzeugte Textur mit weiteren Grafikoperationen verändert wird.

Eine weitere Möglichkeit ist die Szene in ein \emph{FrameBuffer Object} zu
rendern und die erzeugte Textur auf eine vorhandene Geometrie zu projezieren.
Als Beispiel lässt sich der Spiegel nennen. Wird die Szene gespiegelt gerendert
und danach auf eine Ebene projeziert erhält man eine spiegelnde Oberfläche.

Das \emph{FrameBuffer Object} findet vor allem dann Anwendung, wenn eine Szene
nachträglich verändert werden soll. Dieses Verfahren wird auch
\emph{Post-Processing} genannt. Diese Nachbearbeitung dient dazu ein optischen
Effekt zu erzeugen, der dem Anwender ein realeres Bild vermitteln soll.

Depth-of-Field ist ein solcher Effekt, der mit der Hilfe des
\emph{FrameBuffer Objects} realisiert wird. Dabei wird ein Z-Buffer-Test von
der Szene gemacht und in die Textur geschrieben. Die erhaltenen Informationen
stellen den Abstand der Pixel zum Betrachter dar. Je weiter ein Pixel vom
Betrachter weg steht, desto stärker wird dieser weichgezeichnet. Dies erzeugt
den Eindruck, dass Objekte, die weiter weg vom Betrachter stehen, unschärfer
erscheinen.

\subsection{Virtual Reality}

Das jVR-Framework bietet zusätzlich die Möglichkeit eine Art virtueller
Realität zu erzeugen. Dabei werden stereoskopische Bilder erzeugt, die durch
eine entsprechende Brille einen räumlichen Effekt simulieren. Dies wird
durch die Funktion des Multithreading geschafft.

Multithreading bedeutet, dass mehrere Prozesse bzw. Grafikoperationen von der
Engine gleichzeitig durchgeführt werden. Im Falle vom jVR-Framework lassen
sich mehrere Szenen gleichzeitig rendern. Es lassen sich zudem mehrere Anzeigegeräte
ansteuern. Dabei werden die zu erzeugenden Teilbilder parallel gerendert.

Um stereoskopische Bilder zu erzeugen werden 2 voneinander versetzte Kameras
aufgestellt und die Szene simultan aus Sicht der beiden Kameras gerendert. Bei
der Darstellung werden die 2 erzeugten Bilder übereinandergelegt. Mit der schon
erwähnten Brille werden die Bilder aus Sicht des Betrachters zusammengefügt. Es
entsteht ein 3dimensionaler Effekt.

Ein weiterer Teil der virtuellen Realität ist die Funktion des Headtrackings.
Dabei werden die Bewegung des Kopfes vom Betrachter aufgezeichnet und vom
Programm verarbeitet. Je nach Neigung bzw. Drehung des Kopfes wird die
betrachtete Szene angepasst, was dem Betrachter die Illusion vermittelt sich
tatsächlich in der Szene zu bewegen.

\subsubsection{Holodeck}

Das Holodeck ist eine Einrichtung der BHT, in der eine virtuelle Realität
simuliert werden kann. Es ist die bevorzugte Plattform für das jVR-Framework und
mit deren Hilfe entwickelte Anwendungen. Das jVR-Framework wurde mit dem
Gedanken entwickelt, die Technologien, die im Holodeck angeboten werden effektiv
nutzen zu können.

Das Holodeck besitzt eine große Projektionsleinwand. Die Leinwand wird von 2
Beamern mittels Rückprojektion bestrahlt. Das bedeutet, die beiden Beamer
strahlen zunächste das Bild gegen eine große Spiegelfläche und diese projeziert
das Bild letztendlich gegen die Leinwand. Dieses Verfahren wird auch zirkulare
Polarisation genannt.

%TODO: Bild von Projektion

Der Grund für den Einsatz für 2 Beamern ist es stereoskopische Bilder zu
erzeugen. Diese können mit einer Shutterbrille betrachtet werden. An der
Shutterbrille sind mehrere Trackingpunkte befestigt.

Zum Tracking wird im Holodeck das Qualisys-Motion Capture System verwendet. Es
kann mittels einer Kamera die Trackingpunkte orten. Die Bewegungen der Punkte
werden analysiert und auf die Szene übertragen.

Um sich in der Szene zu bewegen, wird die Wii-Mote zusammen mit dem Nunchuk von
Nintendo benutzt. An ihr sind ebenfalls Trackingballs befestigt. In Tabelle
\ref{tab:spec} sind nochmal alle technischen Daten des Holodecks im Überblick.

\begin{table}
\centering
\begin{tabular}{ll}
\toprule
Projektoren: & 2 x DELL 7609WU (DLP) @ 1920 x 1200 \\
Leinwand: & Stewart Screen Wall (3,73m x 2,33m) \\
Headtracking: & Qualisys-Motion Capture System @ 1280 x 1024 \\
Input: & Nintendo Wii-Mote und Nintendo Nunchuk \\
CPU: & Intel QX9650 @ 3,0 GHz \\
RAM: & 4GB DDR3 \\
GPU: & 2 x NVIDIA Geforce 8800GT \\
\bottomrule
\end{tabular}
\caption{Technische Daten des Holodecks}
\label{tab:spec}
\end{table}

\section{Verwendete Standards im jVR-Framework}

Das jVR-Framework setzt sich aus mehreren externen Bibliotheken und Standards
zusammen. In diesem Abschnitt werden die einzelnen Standards betrachtet, die in
jVR Anwendung finden. Zudem wird ein Überblick gegeben, in welcher Form die
Standards ins jVR-Framwork integriert wurden.

\subsection{OpenGL}

Für die 3D-Beschleunigung wird die Grafikbibliothek OpenGL verwendet. \emph{Open
Graphics Library}, kurz OpenGL ist ein offener Grafikstandard, der zur
2D- und 3D-Echtzeit-berechnung verwendet wird.

OpenGL geht aus der von Silicon Graphics Inc. (SGI) entwickelten IRIS GL API
hervor. Ab 1992 wurde OpenGL vom OpenGL ARB (Architecture Review Board)
beaufsichtigt und weiterentwickelt. Heute ist die Khronos Group für die
Weiterentwicklung der API zuständig. Die Khronos Group zählt weit über 100
Mitglieder, darunter AMD, Intel und NVIDIA.

OpenGL wird von nahezu jeder Grafikkarte unterstützt, die richtigen Treiber
vorausgesetzt. Die Besonderheit von OpenGL ist im Vergleich zu Direct3D seine
Plattformunabhängigkeit. Ebenso wie Java lässt sich eine Implementation von
OpenGL auf jedem gängigen Betriebssystem, wie Windows, Mac OS X und Linux
finden. Dies erleichtert Programmierern Anwendungen zu erstellen und in
verschiedenen Umgebungen zu testen.

OpenGL kann als Zustandsautomat verstanden werden. Wird z.B. eine Farbe oder
eine Transformation festgelegt, werden alle daraufhin gezeichneten Objekte mit
der gewählten Farbe koloriert und der festgelegten Transformation transformiert.

OpenGL besaß bis zur Version 1.5 eine Fixed Function Pipeline mit der die
Objekte gerendert werden mussten. Die Fixed Function Pipeline war für die
Beleuchtung und Transformation der Vertices verantwortlich. Erst mit der Version
2.0 ließ sich die Pipeline über die \emph{OpenGL Shading Language} zum Teil
programmieren.

Seit OpenGL 3.0 wurde damit begonnen die meisten Befehle der Fixed Function
Pipeline aus der Spezifikation zu entfernen. Mit der Veröffentlichung von OpenGL
3.1 ist die Fixed Function Pipeline komplett aus der Spezifikation verschwunden.
Die Befehle lassen sich aber weiterhin über OpenGL-Extensions nutzen.

\begin{figure}[ht]
 	\includegraphics[width=\linewidth]{img/pipeline}
	\caption{OpenGL 3.2 Rendering-Pipeline}
	\label{fig:og32}
\end{figure}

Die Abbildung \ref{fig:og32} zeigt die OpenGL Pipeline. Zuerst werden die Attribute
der Vertices an den Vertex-Shader übergeben. Dieser reicht die verarbeiteten Daten
weiter an den Geometry-Shader. Nachdem die Geometrien erzeugt bzw. verarbeitet
wurden wird das Clipping vorgenommen. Nachdem dieser Schritt beendet ist, wird
die erhaltene Szene auf den Bildschirm gemappt. Das bedeutet die Szene wird aus
Sicht der Kamera gerendert und danach über den Rasterizer in die festgelegte
Auflösung überführt.

Die rasterisierte Szene wird dem Fragment-Shader übergeben, der weitere
Verarbeitungsschritte vornimmt. Zum Beispiel werden die Sichtbarkeit der
einzelnen Pixel getestet (Z-Buffer). Zum Schluss werden alle erzeugten Fragmente
zusammengelegt und den FrameBuffer geschrieben. Der FrameBuffer wird entweder
von der Pipeline als Textur weiterverwendet oder in einer Fensterumgebung auf
dem Bildschirm ausgegeben.

\emph{OpenGL Embedded Systems}, kurz OpenGL ES ist eine spezielle
Zusammenstellung der OpenGL API für mobile Geräte. Dadurch lassen sich
Anwendungen auf mobilen Geräten realisieren. Durch die begrenzten Kapazitäten,
wie Arbeitsspeicher und Rechenleistung der mobilen Hardware, ist diese Version
der OpenGL API um einige Funktionen erleichtert worden.

%TODO besser formulieren
Ins jVR-Framework wurden viele häufig benutzten Grundfunktionen implementiert.
Dies dient dazu um die Produktivität zu steigern. Dadurch kommt der Entwickler
so gut wie nie in Kontakt mit reinen OpenGL-Befehlen.

\subsection{GLSL}

Eine weitere Besonderheit des jVR-Frameworks bietet die Unterstützung von
Shadern, die in der \emph{OpenGL Shader Language}, kurz GLSL geschrieben werden.
Shader sind kleine Programme die direkt auf der GPU ausgführt werden. Sie
ersetzen zudem die Fixed Function Pipeline von OpenGL.

GLSL besitzt eine C-ähnliche Syntax und besitzt zudem noch zusätzliche
Datentypen wie Vektoren und Matrizen. Vor der Verwendung müssen die Shader
compiliert werden. Dies bietet den Grafikkartenherstellern den Vorteil, den
Compiler für die eigene Hardware zu optimieren.

GLSL bietet außerdem die Möglichkeit ein besseres Beleuchtungsmodell zu
verwenden, als das von der Fixed Function Pipeline bereitgestellte. Seit Version
3.1 von OpenGL sind Fragment- und Vertex-Shader ein wichtiger Bestandteil der
Spezifikation und müssen zwingend implementiert werden. Durch Verwendung dieser
Shader lassen sich Oberflächeneffekte oder auch Effekte, wie Motion Blur und
Depth of Field erzeugen.

Mit OpenGL 3.2 kommen die Geometry-Shader zur Kernspezifikation hinzu. Diese
Shader ermöglichen es Geometrien direkt auf der Grafikkarte zu erzeugen und zu
verarbeiten. OpenGL 4.0 erweitert die Spezifikation um den Gebrauch von
Tesselation-Shadern. Diese Shader können dazu verwendet vorhandene Polygone in
weitere kleinere Polygone aufzuteilen und so die Oberfläche eines geometrischen
Objekts zu verbessern.

\subsection{JOGL}

Im jVR-Framework wird die \emph{Java Bindings for OpenGL} (JOGL) verwendet, um
OpenGL mit Java nutzen zu können. JOGL stellt eine Wrapperbibliothek dar, die die
OpenGL-Befehle über Javafunktionen ansteuern lässt.

Auf die native OpenGL API wird mit Hilfe des \emph{Java Native Interface} (JNI)
zugegriffen. Der native C-Code ist auf allen großen Systemen wie Windows, Mac OS
X und Linux verfügbar.

JOGL wurde von der Sun Microsystems Game Technology Group entwickelt. Es steht
als Open Source zur Verfügung und wird unter der BSD Lizenz angeboten. Es stellt
zudem eine Referenzimplementierung der Java Specification Request (JSR-231) dar.
JOGL wird möglicherweise in einer späteren Java-Version zu den
Standarbibliotheken mit aufgenommen.

In der Version 2 von JOGL wurden die GLProfile eingeführt. Diese ermöglichen es
dem Entwickler sich für ein OpenGL-Version zu entscheiden. Dadurch kann
bestimmt werden auf welchen Plattformen die erstellten Anwendungen laufen
und es werden zudem Kompatibilitätsprobleme vermieden.

Das jVR-Framework benutzt in diesem Fall das GL2GL3-Profil. Es vereint alle
Funktionen der Profile GL2 und GL3. Das Profil hat die Eigenschaft GL3 konform
zu sein. Es können daher GL3-Anwendungen entwickelt werden, die aber auch auf
Systemen lauffähig sind, die nur OpenGL 2 unterstützen.

% \begin{figure}[ht]
% 	\includegraphics[width=\linewidth]{img/glprofiles.png}
% 	\caption{Test 123}
% \end{figure}

\subsection{jBullet}

Um physikalische Effekte in dem jVR-Framework zu realisieren wird Gebrauch von
jBullet gemacht. jBullet ist eine Bibliothek, die die Möglichkeit bietet
Kollisionen zwischen Objekten zu erkennen und geometrische Objekte mit
physikalischen Eigenschaften zu versehen.

Es kann ein physikalische Umgebung beschrieben werden, die betimmten
physikalische Gesetzen folgt. Zum Beispiel kann die Stärke der Gravitation und
das Gewicht der Körper festgelegt werden. Dadurch lassen sich Szenen entwickeln,
die entweder eine realistische Physik bieten, wie sie auf der Erde vorhanden ist
oder aber mondähnliche Verhältnisse geschaffen werden, in denen die Körper in
trägerer Art und Weise durch die Gegend bewegt werden können.

Die Engine bietet zudem eine Kollisionserkennung. Mit ihrer Hilfe lassen sich
Situationen simulieren, in denen mehrere Objekte miteinander kollidieren. Die
Objekte müssen sich je nach Gewicht und Beschaffenheit korrekt verhalten. Diese
komplexen Berechnungen übernimmt die jBullet-Engine.

Die jBullet-Engine ist eine Portierung der Bullet-Engine. Bullet wurde in C++
entwickelt. Die Bullet-Engine ist eine weit verbreitete Physik-Engine, die
selbst in vielen kommerziellen Projekten Anwendung findet. Sie wurde zum
Beispiel in Filmen wie 2012, Shrek 4 und A-Team verwendet.

Viele der bekannten 3D-Authoring Programme lassen sich mit einem PlugIn
erweitern, das alle Eigenschaften der Bullet-Engine integriert. Damit lassen
sich auch in diesen Programmen physikalisch korrekte Szenen simulieren.

\subsection{COLLADA}

Ein wichtiger Bestandteil des jVR-Frameworks stellt die Benutzung des
3D-Austausch-formats COLLADA dar. Dieses Format ist basiert auf der
Auszeichnungssprache XML. Es dient dazu wichtige Daten zwischen verschiedenen
3D-Programmen zu verwalten. Weiterhin ist es als offener Standard definiert.

Eine COLLADA-Datei setzt sich aus mehreren Bereichen zusammen. Zum einen werden
die Geometrien beschrieben und zum anderen werden die verwendeten Texturen über
Pfade angesteuert. Weitere Bereiche sind Lichter, Materialien und Kameras.

Seit der Version 1.4 können auch physikalische Eigenschaften in COLLADA-Dateien
hinterlegt werden. Dabei können Oberflächeneigenschaften wie Reibung definiert
werden.

Das COLLADA-Format wird von den meisten 3D-Authoring-Programmen
unterstützt, darunter Blender, Autodesk Maya und Autodesk 3D Studio Max.
Außerdem kommt es in Google SketchUp und Google Earth zur Anwendung, was zu
einem Großteil der Verbreitung beigetragen hat.

\chapter{Entwurf des Portal Cullings}

Der Schwerpunkt der Arbeit stellt die Erweiterung des jVR-Frameworks dar. Die
Erweiterung soll es möglich machen, Portale innerhalb einer Szene verwenden zu
können. Die Portale sollen ähnlich wie andere Szeneknoten erschaffen und
transformiert werden können.

Der Entwurf soll noch einmal die wichtigsten Konzepte wiedergeben, die für das
Erschaffen von Portalen notwendig sind. Es werden die verschiedenen
Arten von Portalen und deren besonderen Eigenschaften beschrieben und
erläutert. Weiterhin wird die Verwendung von virtuellen Kameras betrachtet und
welche Rolle sie beim erstellen von Portalen spielen.

\section{Portaltypen}

Es gibt verschiedene Arten von Portalen, die unterschieden werden müssen. Zum
einen gibt es Portale die betreten bzw. durchtreten werden können. Andere
Portale hingegen lassen den Betrachter die Szene aus einem anderen Blickwinkel
betrachten. Daher gibt es 3 besondere Arten von Portalen, die bei der
Implementierung berücksichtigt werden.

Die 3 Typen in denen sich die Portale unterteilen sind zum einen die Tür. Sie
kann durchtreten werden. Über sie lassen sich ohne Probleme andere Räume
betreten. Der zweite Typ von Portalen ist der Spiegel. Er gibt die Szene
gespiegelt wieder, wodurch der Betrachter in der Lage ist die Umgebung hinter
ihm zu sehen. 

Der letzte Portaltyp, der behandelt wird ist der Teleporter. Durch
kann man die Szene betrachten. Dabei sieht man den Ausgang des Teleporters, der
an einer völlig anderen Stelle in der Szene platziert sein kann. Wenn der
Teleporter betreten wird, tritt man aus dessen Ausgang aus und befindet sich an
der Stelle, die zuvor durch den Teleporter gesehen wurde.

Jeder dieser Portaltypen kann von der Höhe und der Breite her verändert werden.
Weiterhin besitzen der Teleporter und der Spiegel eine virtuelle Kamera. Der
Nutzen der virtuellen Kamera wird im Abschnitt \emph{Virtuelle Kameras} näher
erläutert. Als Grundlage der vorgestellten Portaltypen steht eine
Oberklasse, von der alle Portale abgeleitet werden, die Klasse \texttt{Portal}. 

\subsection{Spiegel}

Der Spiegel ist ein Portal, welches nicht durchschritten werden kann. Er dient
vielmehr dazu die Umgebung auf seiner Oberfläche in gespiegelter Form zu
projezieren. Um den Spiegel als solchen darstellen zu können wird zunächst eine
Ebene benötigt. Auf dieser Ebene wird die gespiegelte Szene abgebildet.

Um die Szene richtig auf der Oberfläche darzustellen, muss sie zuvor in ein
\emph{FrameBuffer Object} gerendert werden. Dem Spiegel wird ein virtuelle
Kamera zugeordnet. Diese wird von der Pipeline aufgerufen und rendert
dementsprechend die Szene in das \emph{FBO}.

Die Szene muss vor dem Renderprozess von der Kamera aus der richtigen
Perspektive betrachtet werden. Dies wird dadurch erreicht, indem man die
Transformation der Betrachterkamera $C$ in das Koordinatensystem der
Spiegelebene überträgt. Besitzt die Ebene eine Transformation $M$, wird die
Kameratransformation über die inverse Transformation $M^{-1}$ in das
Objektkoordinatensystem der Ebene übertragen.

Um die Szene auch wirklich zu spiegeln, erhält die virtuelle Kamera zunächst ein
negatives Bildseitenverhältnis, z.B. -4:3. Zusätzlich wird die Kamera mit einer
negativen Skalierung transformiert. Die Transformation $S(1,1,-1)$ stellt die
Skalierung dar, mit der die Kamera transformiert wird (vergleiche auch \cite[S.
33]{jvrMast}).

Die Gleichung \ref{eq:mirTrans} verdeutlicht nocheinmal die Transformationen,
die auf die virtuelle Kamera angewendet werden müssen, um diese als
Spiegelkamera nutzen zu können.

\begin{equation}
{C}_{mirror} := M * S * M^{-1} * C
\label{eq:mirTrans}
\end{equation}

Da in diesem Verfahren eine virtuelle Kamera benutzt wird, bewegt sich diese je
nach Position des Betrachters. Dadurch kann es passieren, dass Geometrien, die
eigentlich hinter dem Spiegel stehen, mitgerendert werden. Um dies zu verhindern
wird der Gebrauch von Clippingebenen gemacht. Man positioniert eine
Clippingebene an der Stelle des Spiegels und verwirft dabei alle Fragmente die
sich dahinter befinden und daher nicht gespiegelt werden können.

Spiegelnde Ebenen finden zum Beispiel dann eine Verwendung, wenn glatte
Oberflächen simuliert werden sollen. Ein Beispiel für eine spiegelnde Oberfläche
ist das Wasser. Wasser spiegelt nicht komplett, sondern ist zum Teil noch
durchsichtig. Der Effekt der Transparenz kann über Modifizieren des Shaders
erreicht werden. Die erzeugte Textur, die beim Rendern entsteht wird dann
durchsichtig gemacht.

\subsection{Teleporter}

Ein weiterer Portaltyp, der umgesetzt werden soll, ist der Teleporter. Der
Teleporter ist ein Portal welches betreten bzw. durchtreten werden kann. Das
bedeutet, schreitet der Betrachter durch den Teleporter, kommt an einer anderen
Stelle der Szene wieder heraus.

Jeder Teleporter benötigt einen Ausgang um genutzt werden zu können. Der Ausgang
wird durch einen zweiten Teleporter repräsentiert. Dieser kann beliebig in der
Szene platziert sein. Um ihn eindeutig dem Eingangsteleporter zuzuordnen, wird
die virtuelle Kamera des einen Teleporters dem anderen Teleporter übergeben und
umgekehrt.

Wenn der Teleporter einen Ausgang zugeordnet bekommen hat, kann durch den
Teleporter die Szene betrachtet werden. Man sieht die Szene durch den Teleport
aus der Perspektive des Ausgangs. Das heißt, würde man sich direkt hinter den
Ausgang des Teleporters stellen und sich in dessen Richtung drehen, sieht man
die Szene so, wie sie im Teleporter zu sehen sein würde. 

Die virtuelle Kamera des Teleporters wird an die Stelle des zugeordneten
Ausgangs platziert. Diese Kamera wird relativ zur Position der Kamera des
Betrachters zum Teleporter auf der Gegenseite ausgerichtet. Je nach Bewegung des
Betrachters bewegt sich auch die virtuelle Kamera.

Die interessanteste Eigenschaft eines Teleporters ist die Fähigkeit der
Teleportation. Ein Teleporter muss prüfen, ob sich eine \texttt{SceneNode} in
einem gewissen Abstand befindet. Ist der Abstand gering genug kann die
\texttt{SceneNode} teleportiert werden. Beim Teleportieren wird die
\texttt{SceneNode} an die Position des festgelegten Ausgangs transformiert.

Um den Abstand des Betrachters bzw. eines Objekts in Form einer
\texttt{SceneNode} zu bestimmen, gibt es 2 Möglichkeiten. Die erste Möglichkeit
besteht darin, den Mittelpunkt eines Objekts bzw. der Kamera zu nehmen und dann
den Abstand zur Ebene des Teleporters auszurechnen. Dazu nimmt man die
Hessesche Normalform, siehe Gleichung \ref{eq:popldist}.

\begin{equation}
\textbf{HNF: }
\frac{{n}_{1}{x}_{1} + {n}_{2}{x}_{2} + {n}_{3}{x}_{3}}{\sqrt{{n}_{1}^2 +
{n}_{2}^2 + {n}_{3}^2}} = d
\label{eq:popldist}
\end{equation}

Eine andere Möglichkeit den Abstand zu bestimmen ist über die \emph{Bounding
Box} des Portals. Dabei wird der Mittelpunkt des Objekts bzw. der Kamera ins
Objektkoordinatensystem des Portals übertragen. Danach wird getestet, ob sich
der Punkt innerhalb der BB befindet. Da die Ebene flach ist, sind der maximale
und minimale Z-Wert der BB 0. Es reicht den maximalen Wert zu erhöhen um eine
3dimensionale BB zu erzeugen. Sobald der Mittelpunkt innerhalb liegt, wird das
Objekt bzw. die Kamera teleportiert.

\subsection{Türen}

Der letzte umzusetzende Portaltyp ist die Tür. Sie kann ebenfalls durchtreten
werden. Eine Tür ist ein Durchgang der mehrere Räume miteinander verbindet.
Diese Räume können beliebig angeordnet sein. Die Größe der Tür lässt sich
bestimmen, wodurch die Tür entweder ein kleines Loch sein kann oder aber einen
ganzen Durchgang schafft.

Türen eignen sich besonders dafür, um Portal Culling durchzuführen. Wie jedes
Portal besitzt auch die Tür eine \emph{Bounding Box}. Die \emph{Bounding Box}
umfasst die gesamte Tür. Anhand der BB der Tür kann bestimmt werden, welche
Bereiche im Raum für den Betrachter sichtbar sind und welche nicht.

Dazu wird von dem Betrachter ausgehend ein Frustum aufgespannt. Wenn das Frustum
auf die \emph{Bounding Box} der Tür trifft, wird die Schnittmenge aus dem
Frustum des Betrachters und aus der BB der Tür gebildet. Daraus wird ein neues
Frustum gebildet. Dieses Frustum prüft im benachbarten Raum, welche Objekte bzw.
dessen \emph{Bounding Boxes} sich innerhalb befinden. Alle Objekte außerhalb des
Frustums werden nicht gerendert.

Weiterhin können ganze Räume ausgeschlossen werden vom Renderprozess. Dies
geschieht wenn der Betrachter gerade nicht auf eine Tür blickt. Dabei befindet
sich die \emph{Bounding Box} der Tür nicht im \emph{View Frustum} des
Betrachters. Ist dies der Fall wird die Tür, genau wie die benachbarten Räume,
nicht vom Renderverfahren berücksichtigt.

Eine Tür wird innerhalb einer Wand platziert. Man wählt dafür eine Wand aus, in
der die Tür platziert werden soll. Danach wird die Wand neu zusammengesetzt. Das
bedeutet, es werden einzelne Wandstücke erzeugt, die sich genau um die Tür herum
anordnen. Dadurch entsteht ein Durchgang zu einem anderen Raum. 

Da Türen 2 Räume (auch als Zellen bezeichnet) miteinander verbinden können, kann
festgelegt werden welche 2 Räume miteinander verbunden sind. Zum einen wird
der Tür der aktuelle Raum übergeben, den sie mit einem neuen Raum verbinden
soll. Dazu wird der daraufhin erzeugte Raum als \emph{Nachbar} ebenfalls
der Tür übergeben. Dadurch kann ermittelt werden, welche Räume über eine
bestimmte Tür verbunden sind.

Mit verschiedenen Arten von Türen lassen sich interessante Szenen bauen, die
innerhalb eines riesigen Gebäudes spielen. Dabei muss auf die größe des Gebäudes
keine Rücksicht genommen werden, da die nicht sichtbaren Teile des Gebäudes
nicht gerendert werden, und somit keine Rechenleistung beanspruchen.

\section{Virtuelle Kameras}

Die virtuellen Kameras sind ein wichtiger Bestandteil der Portale. Für jede
virtuelle Kamera wird ein \texttt{FrameBufferObject} angelegt. Das
\texttt{FrameBufferObject} ist die Repräsentation der Szene aus Sicht der
virtuellen Kamera. Wird die Szene aus Sicht der Betrachterkamera gerendert,
werden alle erzeugten \texttt{FrameBufferObjects} auf das jeweilige Portal
projeziert.

Um das \texttt{FrameBuffer Object} nutzen zu können muss über die Pipeline erst
eines erzeugt werden. Ist dies geschehen, wird auf die virtuelle Kamera
geschaltet, um die Szene darüber zu rendern. Dann der Pipeline wiederum
mitgeteilt, die gerenderte Szene in das FBO zu rendern und nicht auf den
Bildschirm zu projezieren. Im weiteren Schritt bekommt der Shader, der ein Teil
des Materials der Portale ist, das FBO als Textur übergeben und bildet diese
dann auf dem jeweiligen Portal ab.

Es gibt 2 Portaltypen, die eine virtuelle Kamera besitzen. Zum einen
\texttt{Teleporter} und zum anderen \texttt{Mirror}. Diese beiden Portaltypen
benötigen die virtuelle Kamera um die Szene aus einer gewissen Position zu
rendern im Anschluss auf dem jeweiligen Typen anzuzeigen.

\section{Portallisten}

Um alle Portale innerhalb einer Szene rendern zu können, kommen die Portallisten
zum Einsatz. Sie sammeln die Portale innerhalb einer Liste. Wenn ein Portal
erzeugt wird, wird es gleich daraufhin der \texttt{PortalList} hinzugefügt. Sie
ruft für jedes Portal die jeweilige Render-Funktion auf erzeugt dementsprechend
das aktuelle Portal.

Die Portallisten verwalten die hinzugefügten Portale. Sie rufen für jeden neuen
Frame die Methode auf um die Transformation der virtuellen Kamera zu
aktualisieren. Dabei wird der Portalliste die Kameratransformation der
Betrachterkamera übergeben und weiterverarbeitet. Dies bietet den Vorteil nicht
jedes Portal einzeln aufrufen und verwalten zu müssen.

Teleporter sind zwar mit der \texttt{PortalList} kompatibel, müssen aber
trotzdem gesondert behandelt werden. Da Teleporter zusätzlich mit physikalischen
Objekten umgehen können, muss eine zusätzlich Methode verwendet werden, die die
verschiedenen Objekte nacheinander testen und bei Bedarf transformiert. Zu
diesem Zweck wird die \texttt{TeleportList} verwendet, eine abgeleitete Form der
\texttt{PortalList}.

\section{Zellen}

Zellen stellen in sich abgeschlossene Räume dar. Sie erben von der Klasse 
\texttt{GroupNodes} und können daher weitere Szeneknoten enthalten. Zellen
lassen sich über Türen miteinander verbinden. Jede Zelle wird durch 4
Wände, eine Decke und einen Boden definiert, wobei die Breite, Länge und Höhe
einer Zelle variieren kann.

Innerhalb einer Zelle befindet sich zudem eine \texttt{PointLightNode}, um den
inneren Bereich der Zelle richtig ausleuchten und darin positionierte Objekte
richtig darstellen zu können. Eine Zelle lässt sich farblich anpassen, oder aber
mit einer Textur versehen.

Eine Zelle stellt eine \texttt{GroupNode} dar. Daher lässt sie sich nach
Belieben transformieren, zusammen mit allen enthaltenen Kindknoten. Zellen
können somit beliebig platziert werden und daraufhin mit einer benachbarten
Zelle über eine Tür verbunden werden. Die Information ob 2 Zellen benachbart
sind oder nicht wird in der Tür gespeichert.

\chapter{Praktische Umsetzung}

Dieses Kapitel befasst sich mit der Implementierung der Erweiterung für das
jVR-Framework. Im ersten Teil werden die Portal-Klassen und ihre verschiedenen
Methoden genauer betrachtet. Der zweite Teil stellt den Ansatz vor, mit dem das
Portal Culling umgesetzt wurde.

\section{Paketstruktur}

Die Paketstruktur stellt eine Ergänzung zum jVR-Framework dar und fügt sich in
dessen Struktur ein. Die Pakete unterteilen sich daher logisch und überschneiden
sich nicht mit Paketen von jVR. Das Oberpaket \texttt{portals} enthält alle
weiteren Unterpakete, sowie die verschiedenen Typen von Portalen, die im Laufe
der Implementierung entstanden sind.

Das Paket \texttt{culling} besteht aus den Klassen die für Portal Culling von
Bedeutung sind. Mit ihnen können Zellen erzeugt werden, also Räume die
untereinander verbunden sind. Zudem lassen sich verschiedene Arten von Wänden
erzeugen, über die Klasse \texttt{Wall}.

Das \texttt{tests}-Paket ist mit Anwendungen gefüllt, die das Verhalten der
einzelnen Portaltypen testet und demonstriert. Die Tests sind ein wichtiger
Bestandteil für die Entwicklung der Portale und ihren unterschiedlichen
Implementierungen.

Ein Unterpaket des \texttt{tests}-Pakets stellt das \texttt{physics}-Paket dar.
Die Tests innerhalb des Paketes dienen dazu, das Verhalten zwischen
physikalischen Objekten und den Portalen zu untersuchen. Sie werden mit
Komponenten der jBullet-Bibliothek erweitert um physikalisch korrekte Bewegungen
und Kollisionserkennung zu erzeugen.

Das letzte Paket ist das \texttt{util}-Paket. Es enthält alle Hilfsklassen die
für den Umgang von Portalen nötig sind. Außerdem bietet es eine
Standardtestklasse an, die von allen Testklassen für die Portalfunktionen
abgeleitet wird.

\section{Portalimplementierung}

Da es mehrere Typen von Portalen gibt, bietet es sich an eine Oberklasse zu
implementieren von der alle konkreten Portaltypen abgeleitet werden können.
Das Entwurfsmuster, welches als Grundlage dient ist das sogenannte
Kompositionsmuster. 

Ähnlich der Szeneknoten im jVR-Framework, die alle von der Oberklasse
\texttt{SceneNode} abgeleitet sind, werden die Portal-Unterklassen von der
Oberklasse \texttt{Portal} abgeleitet. Sie selbst erbt von der Klasse
\texttt{GroupNode} und kann daher mehrere Kindknoten beinhalten.

\begin{figure}[h]
\includegraphics[width=\linewidth]{img/uml_portal}
\caption{Der Implementierung der Portale liegt das Kompositionsmuster als
Grundlage vor. \texttt{Portal} erbt von \texttt{GroupNode} und kann somit
weitere Kindknoten enthalten. \texttt{Portal} fungiert als Oberklasse aller
anderen Portalklassen}
\label{fig:port_uml}
\end{figure}

Dadurch dass die Klasse \texttt{Portal} von der Klasse \texttt{GroupNode} erbt,
übernimmt sie alle Eigenschaften eines Szeneknoten. Das bedeutet eine
Unterklasse der \texttt{Portal}-Klasse lässt sich ohne Probleme transformieren
und in einer Szene platzieren.

Die Klasse \texttt{Portal} ist eine abstrakte Oberklasse. Die Getter und Setter
für Höhe und Breite eines Portals sind vordefiniert und müssen nicht noch einmal
implementiert werden. Zudem benötigt ein \texttt{Portal} die Pipeline und einen
Namen in Form eines Strings.

Die Pipeline wird benötigt, da die Szene zum Beispiel für den
\texttt{Teleporter} erst in ein \emph{FrameBuffer Object} geschrieben werden
muss und erst danach die eigentliche Szene gerendert werden kann, siehe auch
Listing \ref{lst:exa}.

Jedes Objekt vom Typ \texttt{Portal} besitzt eine virtuelle Kamera vom Typ
\texttt{CameraNode}. Dies ist vor allem in den Klassen \texttt{Teleporter} und
\texttt{Mirror} von Bedeutung, da diese für das Rendering der benötigt wird.

Weiterhin stellt die \texttt{Portal}-Klasse 2 abstrakte Methoden bereit. Zum
einen die \texttt{update}-Methode. Sie wird jedes Mal aufgerufen, wenn ein neuer
Frame gezeichnet wird und dient zum Beispiel dazu die Kameraposition zu
aktualisieren. Die zweite Methode ist \texttt{render}. Sie dient dazu die Szene
im AMBIENT und LIGHTING-Pass zu rendern und danach in das \emph{FBO} zu
schreiben.

\subsection{Teleporter-Klasse}

Die Klasse \texttt{Teleporter} ist in der Lage den Betrachter von einer Position
zur anderen zu bewegen. Dazu sind 2 \texttt{Teleporter} notwendig. Für das
Teleportieren wird die Methode \texttt{teleport} verwendet. Ihr wird eine
\texttt{SceneNode} übergeben, sowie ein \texttt{double}-Wert, der die
Bewegungsgeschwindigkeit der Kamera bzw. des Objekts repräsentiert.

In der \texttt{init}-Methode wird das \emph{FrameBuffer Object} erzeugt. Die
Methode sorgt dafür, dass die virtuelle Kamera in die Textur des jeweiligen
Teleporters schreibt. Dazu wird jedem Teleporter ein eindeutiger Name
zugewiesen, damit die verschiedenen \emph{FBOs} dem passenden Teleporter
zugeordnet werden können.

Jeder Teleporter besitzt eine Variable vom Typ \texttt{Portal} mit dem Namen
\texttt{portalExit}. Diese Variable wird initialisiert, wenn 2 Teleporter
miteinander verbunden werden. Sie wird dazu benötigt, um auf die nötigen
Transformationen des Teleporterausgangs zugreifen zu können.

Um den Betrachter teleportieren zu können, wird zunächst die
\texttt{update}-Methode aufgerufen. Diese Methode wird aufgerufen, sobald ein
neuer Frame gezeichnet werden muss. Die übergebene \texttt{CameraNode} wird in
das Objektkoordinatensystem des Portals überführt, siehe auch Gleichung
\ref{eq:obj}. 

Danach wird die Transformation gespeichert und wiederum mit der Transformation
des \texttt{portalExit} multipliziert. Als letzten Schritt wird die
Transformation um 180° auf der y-Achse gedreht. Die daraus resultierende
Transformation wird auf die virtuellen Kamera angewendet. Die
\texttt{update}-Methode ruft zudem die Methode \texttt{getPickPoint} auf.

Die Methode \texttt{getPickPoint} bekommt als Argument eine \texttt{SceneNode}
und deren Bewegungsgeschwindigkeit übergeben. Sie prüft, ob sich der
Szeneknoten innerhalb der definierten \emph{Bounding Box} befindet. Ist dies der
Fall wird darauf die \texttt{teleport}-Methode aufgerufen. 

Dies geschieht, indem der Szeneknoten ins Koordinatensystem des Portals
transformiert wird. Daraufhin wird aus der Transformation der
Translations-Vektor gewonnen und mit den minimalen sowie maximalen Werten der
\emph{Bounding Box} verglichen.

In der \texttt{teleport}-Methode wird die \texttt{SceneNode} ins
Objektkoordinatensystem transformiert. Dazu muss die inverse Transformation
des aktuellen Teleporters genommen werden und mit der Transformation der
\texttt{SceneNode} verrechnet werden.

\begin{equation}
{T}_{} := {T}_{Teleporter}^{-1} * {T}_{SceneNode}
\label{eq:obj}
\end{equation}

Aus der gewonnenen Transformation $T$ werden die Translations- 
und die Rotationstransformation extrahiert. Danach wird die Transformation des
\texttt{portalExit} mit der Rotationstransformation verrechnet, siehe auch
Gleichung \ref{eq:exit}. Dies wird gemacht, sodass der Szeneknoten mit der
gleichen Rotation aus dem Teleporterausgang austritt, mit der den
Teleportereingang betreten hat.

\begin{equation}
{T}_{Temp} := {T}_{Exit} * {T}_{Rotation}
\label{eq:exit}
\end{equation}

Danach wird der Knoten mit einem Offset in die z-Richtung transformiert. Dies
hat den Grund, dass der Szeneknoten vor dem Portal und dessen \emph{Bounding
Box} zu landen soll und nicht darin. Wäre dies der Fall würde der Szeneknoten
bei Austritt umgehend wieder teleportiert werden.

\begin{equation}
{T}_{Temp} := {T}_{Temp} * {T}_{\mathit{Offset}}
\label{eq:zOff}
\end{equation}

Um zu verhindern, dass der Knoten am Ende nicht auf der falschen Seite des
Teleporterausgangs austritt, wird Knoten zusätzlich um 180° an der y-Achse
gedreht. Würde dieser Schritt ausgelassen werden, würde der Szeneknoten hinter
dem Ausgang des Portals landen und nicht an der Stelle, die durch das Portal zu
sehen ist.

\begin{equation}
{T}_{Temp} := {T}_{Temp} * {T}_{RotY180Deg}
\label{eq:rot180}
\end{equation}

Die extrahierte Translationstransformation bestimmt den genauen Eintrittspunkt
des Teleporter und wird ebenfalls mit der Transformation verrechnet. Dies
bedeutet, tritt der Knoten im oberen rechten Bereich des Teleporters ein, wird
der Knoten an genau dieser Stelle auch beim Ausgang austreten. Dies verleiht dem
Teleport einen weicheren Übergang.

\begin{equation}
{T}_{Temp} := {T}_{Temp} * {T}_{Translation}
\label{eq:trans}
\end{equation}

Zum Schluss wird die Transformation dem zu teleportierenden Szeneknoten
übergeben. Somit wurde der Szeneknoten erfolgreich teleportiert.

\subsubsection{PortalConnector}

Um die Teleporter miteinander zu verbinden wird die Helferklasse
\texttt{PortalConnector} aufgerufen. Sie bietet nur statische Methoden zur
Verfügung und muss daher nicht instanziiert werden. Über die statische
Methode \texttt{connect} werden 2 Teleporter miteinander verbunden. Dazu werden
2 Teleporter-Objekte übergeben, die verbunden werden sollen.

Die Verbindung zwischen den Teleportern wird hergestellt, indem den jeweiligen
Teleporter die virtuelle Kamera des anderen Teleporters übergeben wird. Jeder
Teleporter besitzt eine Klassenvariable \texttt{portalExit} vom Typ
\texttt{Portal}. Über diese Variable lässt sich die Methode \texttt{getCamera}
aufrufen, die die virtuelle Kamera des jeweiligen Teleporters zurückgibt.

Dadurch lassen sich in den jeweiligen \texttt{Teleportern} die virtuellen
Kameras des zugeordneten Gegenstücks aufrufen und konfigurieren. Bei Bedarf kann
zum Bespiel die Kameratransformation ändern, oder aber das Seitenverhältnis
anpassen.

\subsubsection{Physik}

Um mit physikalischen Objekten umgehen zu können, wird die jBullet-Bibliothek
verwendet. Jedem Objekt mit physikalischen Eigenschaften wird ein
\texttt{RigidBody} zugeordnet. Ein \texttt{RigidBody} ist ein Körper der unter
dem Einfluss von Physik reagiert. Die Physik, die den Körper beeinflusst ist in
der Klasse \texttt{MyPhysics} definiert. Sie verwaltet alle Kollisionsobjekte
und physikalischen Eigenschaften innerhalb der Szene.

\texttt{Teleporter} kann ebenfalls physikalische Objekte teleportieren. Dazu
müssen diese in einer \texttt{GroupNode} gesammelt werden. Die
\texttt{GroupNode} mit den Physikobjekten wird den Teleportern über die
\texttt{update}-Methode übergeben. 

Die Szeneknoten in der \texttt{GroupNode} werden genau wie die Kamera
transformiert und treten danach aus dem Portalausgang aus. Jeder einzelne
Szeneknoten wird überprüft, ob er sich innerhalb der \emph{Bounding Box} des
Teleporters befindet. Die Transformation wird erst dann angewendet, wenn dies
der Fall ist.

Man muss bei dem Umgang mit Physik beachten, dass die \texttt{SceneNode} von der
\texttt{teleport}-Methode allein teleportiert wird. Das bedeutet, der
\texttt{RigidBody}, der zur \texttt{SceneNode} zugeordnet ist, nicht
transformiert wird. Um dieses Problem zu umgehen, muss die Transformation des
\texttt{RigidBody} mit dem der \texttt{SceneNode} angepasst werden.

\subsection{Mirror-Klasse}

Die \texttt{Mirror}-Klasse ist die Repräsentation eines Spiegels. Sie kann die
Szene reflektieren. Der Konstruktor der Mirror-Klasse initialisiert das
Material, mit der die Spiegelreflexion erfolgt. Dieses Material besteht aus
einem Shaderprogramm, welches die erzeugte Textur umkehrt. Die virtuelle Kamera
wird über die Methode \texttt{getCamera} aufgerufen und das Seitenverhältnis
wird über \texttt{setAspectRatio} auf den Wert -4:3 geändert.

Genau wie der \texttt{Teleporter}, besitzt die \texttt{Mirror}-Klasse eine
Methode \texttt{update}. Ihr wird ein \texttt{CameraNode}-Objekt von der
Betrachterkamera übergeben. Dies hat den Grund, dass die Spiegelung sich je nach
Betrachterposition ändert.

Zuerst wird die Transformation der Kamera mit der inversen Transformation des
\texttt{Mirror}-Objekts multipliziert. Nachdem die Kameratransformation in das
Objektkoordinatensystem überführt wurde, wird die neue Transformation mit einer
negativen Skalierung auf $z$ transformiert.

In der \texttt{Mirror}-Klasse ist ein statischer Konstruktor definiert. Dieser
Konstruktor lädt das benötigte Shaderprogramm in die globale Variable
\texttt{shaderProg}. Diese wird dann im \texttt{Mirror}-Konstruktor dem
ShaderMaterial übergeben. Der statische Konstruktor bietet den Vorteil die
Variable für alle \texttt{Mirror}-Objekte einmalig zu initialisieren, da sich
der Shader zwischen den einzelnen Objekten nicht unterscheidet.

\subsection{PortalList}

Um alle Portale verwalten zu können wird die \texttt{PortalList}-Klasse genutzt.
Sie enthält eine \texttt{ArrayList} mit dem generischen Typ \texttt{Portal}. Da
es sich bei der \texttt{PortalList} nur um eine Hilfsklasse handelt, muss von
ihr keine Instanz erzeugt werden. Alle angebotenen Methoden werden statisch
aufgerufen.

Die \texttt{add}-Methode fügt die Portale der Liste hinzu. In jedem Konstruktor
eines Portals wird diese Methode aufgerufen. Somit wird ein Portal nach der
Initialisierung sofort der \texttt{PortalList} hinzugefügt. Das bietet den
Vorteil die Portale nicht manuell hinzufügen zu müssen, da die Portale die zur
Szene hinzugefügt werden sowieso gerendert werden sollen.

Über die \texttt{remove}-Methode lassen sich einzelne Portale wieder aus der
Liste entfernen. Dies kann zum Beispiel dann hilfreich sein, wenn während der
Laufzeit ein Portal aus der Szene entfernt wird, kann daraufhin die Methode
aufgerufen werden. Dadurch muss das Portal nicht unnötig gerendert werden.

Die \texttt{PortalList} verfügt eine Methode \texttt{render}. Diese wird
aufgerufen wenn die Pipeline erzeugt und konfiguriert wurde. Dies hat den Grund,
dass erst die Szene mit leeren Portalen gerendert werden muss. Nachdem die
Portale gerendert wurden, wird für jedes Portal die \texttt{render}-Methode
aufgerufen, die jedem Portal sein \emph{FrameBuffer Object} zuordnet und
letztendlich auf dessen Oberfläche rendert.

Die \texttt{update}-Methode der \texttt{PortalList} wird innerhalb der
Endlosschleife der Anwendung aufgerufen. Das heißt, sie wird für jeden neu
erzeugten Frame aufgerufen. Je nach Portaltyp wird die Transformation der 
virtuelle Kamera aktualisiert und neu gesetzt. Der \texttt{update}-Methode wird
eine \texttt{CameraNode} und eine \texttt{float}-Variable übergeben.

\subsection{Beispiel einer Portalanwendung}

\begin{lstlisting}[captionpos=b,caption={Struktur einer Portalanwendung},label=lst:exa] 
public static void main(String[] args){ 
	// create root node
	GroupNode root = new GroupNode();
	// some scene nodes added ...
	// camera and light node added ...
	CameraNode cam = new CameraNode("cam", 4/3f, 60);
	
	// create Pipeline
	Pipeline p = new Pipeline();
	
	// create teleporter
	// needs pipeline for further rendering
	Teleporter portal1 = new Teleporter(p, "portal1");
	
	// create another teleporter
	Teleporter portal2 = new Teleporter(p, "portal2");
	
	// connect both teleporter
	PortalConnector.connect(portal1, portal2);
	
	// some pipelinecommands ...
	
	// let the list render the scene into the portals
	PortalList.render();
	
	// create render window
	RenderWindow win = new NewtRenderWindow(p);
	
	// create viewer
	Viewer v = new Viewer(win)
	
	// render loop
	while(v.isRunning())
	{
		long start = System.currentTimeMillis();
		
		// render next frame
		v.display();
		
		// move speed of the camera
		double moveSpeed = (
			System.currentTimeMillis()	
			- start) * 0.005f;
		
		// upate scene rendered in the portals
		PortalList.update(camera, moveSpeed);
	}
}
\end{lstlisting}

\section{Umsetzung des Cullings}

Das Culling wurde mit mehreren Klassen umgesetzt. Zum einen wurde die
\texttt{Door}-Klasse realisiert, die für die Tür in einem Raum steht und sich
beliebig in Breite und Höhe verändern lässt. Zum anderen die
\texttt{Cell}-Klasse, die für einen Raum steht und die über die
\texttt{Door}-Klasse mit weiteren Räumen verbunden werden kann.

\subsection{Door-Klasse}

Die \texttt{Door}-Klasse repräsentiert die Tür, durch die ein Raum betreten oder
betrachtet werden kann. Sie wird mit einem eindeutigen Name in Form eines
\texttt{Strings} und mit einer Breite sowie einer Höhe definiert. Wie alle
Portaltypen erbt sie von der Oberklasse \texttt{Portal}.

Im Konstruktor der Klasse wird die \texttt{ShapeNode} mit der Höhe und der
Breite skaliert, um die richtigen Maße für die Tür einzustellen. Zudem wird die
\texttt{SceneNode} des Portals aus der Tür entfernt. Das hat den Grund, die
\texttt{SceneNode} nicht in der Szene darstellen zu müssen.

Die Tür ist in diesem Fall eine freie Fläche, sie soll nicht sichtbar sein,
sondern der Betrachter soll durch die in einen anderen Raum blicken können. Die
\emph{Bounding Box} der Tür ist wichtig, da über sie die maximalen und minimalen
$x$, $y$ und $z$ Werte bestimmt und verwendet werden können. Diese werden für
das bilden des neuen Frustums benötigt, wenn der Betrachter durch die Tür
blicken sollte.

Der Tür werden mehrere Räume vom Typ \texttt{Cell} zugeordnet. Zum einen die
aktuelle Zelle, der eine Tür hinzugefügt werden soll. Dies geschieht über die
\texttt{setCell}-Methode. Zum anderen die Nachbarzelle, die angefügt wird sobald
die Tür in der Zelle platziert wurde. Über die Methode \texttt{setNeighbor}
lässt sich die Nachbarzelle festlegen.

\subsection{Cell-Klasse}

Die Räume, die beim Culling benötigt werden, werden auch als Zellen bezeichnet.
Sie werden in der Implementierung von der Klasse \texttt{Cell} repräsentiert.
Eine Zelle setzt sich aus 6 Wänden zusammen. Alle Wände der Zelle sind vom Typ
\texttt{Wall}. Die Zelle lässt sich in Länge, Breite und Höhe über die Variablen
\texttt{length}, \texttt{width} und \texttt{height} verändern. Zudem lässt sich
noch eine Farbe festlegen, in der die Zelle erscheinen soll.

Die \texttt{Cell}-Klasse besitzt wie alle \texttt{SceneNodes} eine
\emph{Bounding Box}. Über die \emph{Bounding Box} der Zelle lässt sich
ermitteln, ob sich der Betrachter innerhalb der Zelle befindet oder nicht. Dies
geschieht über die \texttt{contains}-Methode. Ihr wird ein
\texttt{SceneNode}-Objekt übergeben. Das \texttt{SceneNode}-Objekt wird mittels
der inversen Transformation der Zelle in das Objektkoordinatensystem übertragen.
Dadurch lässt sich die Position der \texttt{SceneNode} innerhalb der Zelle
ermitteln. Ist diese innerhalb der Zelle wird \texttt{true} zurückgegeben und
ist sie außerhalb \texttt{false}.

Die verschiedenen Wände 

\subsubsection{CellList}

\subsubsection{Wall-Klasse}

\section{Optimierungen}

\section{Verwendete Entwicklungswerkzeuge}

\chapter{Ergebnisse und Bewertung}

\chapter{Zusammenfassung und Ausblick}

\bibliography{lit}
\bibliographystyle{apalike}
\addcontentsline{toc}{chapter}{Literaturverzeichnis}

\end{document}