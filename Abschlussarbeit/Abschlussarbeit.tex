\documentclass[11pt]{scrreprt}
\parindent 0pt
\parskip 11pt

\usepackage[latin1]{inputenc}
\usepackage[ngerman]{babel}
\usepackage[colorlinks=false,pdfborder={0 0 0}]{hyperref}
\usepackage{cite}
\usepackage{color}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{scrhack}
\DeclareGraphicsExtensions{.png}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{darkred}{rgb}{0.5,0,0}

\begin{document}
\lstset{language=Java,
		basicstyle=\ttfamily,
		numbers=left,
		frame=single,
		commentstyle=\color{darkgreen},
		keywordstyle=\color{darkred},
		stringstyle=\color{blue}}

\pagenumbering{Roman}
\tableofcontents

\chapter{Einleitung}
\pagenumbering{arabic}

In der heutigen Zeit haben sich viele elektronische Unterhaltungsmedien
entwickelt und sind im Alltag nicht mehr wegzudenken. Ein besonders großer
Industriezweig, der sich in den letzten Jahren rasant entwickelt hat, ist die
Videospielindustrie.

In kürzester Zeit haben sich aus pixelbasierten 2D-Grafiken komplexe 3D-Welten
entwickelt, die kaum von realen Bilder zu unterscheiden sind. Möglich wird all
dies durch immer leistungsfähigere Hardware, die von Jahr zu Jahr immer mehr
Performance im Bereich der 3D-Echtzeitberechnung bringt.

Um die Hardware auch für die Entwickler programmierbar zu machen, stehen mehrere
Grafik APIs zur Verfügung. Die beiden bekanntesten sind Microsofts Direct3D und
die freie Grafikbibliothek OpenGL. OpenGL ist ein wichtiger Bestandteil dieser
Arbeit und wird im späteren Verlauf der Arbeit näher betrachtet.

Mit Hilfe dieser Grafikbibliotheken lassen sich, mit wenig Aufwand
3D-Anwendungen programmieren. Der Aufwand steigt je nach Komplexität des
Projekts. Heutzutage werden 3D-Anwendungen, wie zum Beispiel Spiele nicht von
Grund auf neu entwickelt. In diesem Fall kommen die Grafik-Engines zum Einsatz.

Grafik-Engines unterstützen von vornherein wichtige Konzepte zur
Spieleentwicklung, wie zum Beispiel Shaderunterstützung, fertige Komponenten
wie Kamera, Szenegraphen usw.. Die verbreitesten Engines sind die Unreal Engine
von Epic Games, die Source Engine von Valve Corporation und die Cry-Engine von
Crytek.

Im Mittelpunkt dieser Arbeit steht das jVR-Framework. Es ist komplett auf Java
aufgebaut. Als Grafikbibliothek wird OpenGL in Form von JOGL verwendet. Ein
Besonderheit des jVR-Frameworks ist die Möglichkeit, Virtual Reality Systeme
zu entwerfen. Die Beuth Hochschule für Technik besitzt ein Holodeck, wo
mehrere Anwendungen demonstriert werden, die mit jVR entwickelt wurden.

Ein weiteres Konzept in der Spieleentwicklung ist das sogenannte Portal Culling
bzw. Portal Rendering. Besonders populär wurde diese Art des Renderings mit der
Veröffent-lichung des Spiels Portal von Valve Corporation. Darin wird der
Spieler in die Lage versetzt Portal von Hand zu erschaffen und so Orte zu
erreichen, die zuvor unerreichbar schienen.

Um Entwicklern die Möglichkeit zu bieten, selbst ein Spiel im Stile von Portal
entwickeln zu können, besteht der Schwerpunkt dieser Arbeit darin das 
jVR-Framework um die Funktion zu erweitern, Portale nach Belieben zu erschaffen
und zu platzieren. Portale können dann wie Kameras oder Objekte in Szenen
verwendet werden.

Portale lassen sich vielseitig einsetzen. Sie können zum Beispiel dazu genutzt
werden große Entfernungen in kürzester Zeit zu überbrücken. 

\chapter{Aufgabenstellung}

Die Aufgabe besteht darin eine Erweiterung in das jVR-Framework in Form einer
Portal-Funktion zu integrieren. Man wird dadurch in die Lage versetzt innerhalb
eines vordefinierten Raumes durch verschiedene Portale zu treten und bestimmte
Orte innerhalb des Raumes zu erreichen.

Weiterhin wird man in die Lage versetzt Portale bei Bedarf selbst zu erschaffen
und zu positionieren. Außerdem kann man bestimmte Objekte nehmen und durch die
Räume bewegen. Dabei sollen sich diese Objekte physikalisch korrekt verhalten
und mit in die Portal-Funktionalität integriert werden. Das soll heißen, falls
ein Objekt durch ein Portal befördert wird, soll es aus dem entsprechenden
Gegenstück auch entsprechend der Erwartung physikalisch korrekt austreten.

Als Ansatz dient das Portal-Rendering-Verfahren, welches in der Computergrafik
dafür benutzt wird geschlossene Räume, die in verschiedene Segmente aufgeteilt
sind untereinander zu verbinden und je nach Sichtbarkeit zu rendern. Dabei
werden Lichteinfall sowie Schatteneinfall, sowie die Position der Kamera
berücksichtigt.

Durch die Erweiterung des jVR-Frameworks, welches speziell mit dem Gedanken
implementiert wurde um das Holodeck der BHT und stereoskopische
Rendering-Verfahren zu benutzen um 3dimensionale Szenen erzeugen die mit einer
3D-Brille betrachtet werden. Weitere Möglichkeiten bietet das
Headtracking-Verfahren um die Gesichtsposition zu ermitteln und so die
Perspektive dementsprechend anzupassen.

Das Ergebnis der Aufgabe wird eine Beispielimplementierung sein, die zum Einen
die Erschaffung von Portalen demonstriert. Des Weiteren werden verschiedene
Beispielszenen erstellt, in denen das Portal-Rendering und Lösungen
verschiedener Problemfälle, die bei dem vorgestellten Rendering-Verfahren
auftreten können vorgestellt werden.

\section{Motivation}

Im Rahmen des Masterkurses \emph{Computer Graphics and Effects} an der Beuth
Hochschule für Technik wird den Teilnehmern ein Verständnis für
3D-Echtzeitprogrammierung vermittelt. Der Kurs befasst sich vor allem mit dem
Einsatz von Shadern und deren Programmierung. Im Laufe des Kurses wird eine
3D-Echtzeitanwendung entwickelt. Zur Erstellung der Anwendung wird das
jVR-Framework verwendet.

Das Framework besitzt bereits eine Vielzahl an Möglichkeiten 3D-Anwendungen zu
erstellen. Es können Shader eingesetzt, Modelle geladen und transformiert und
Schatten erzeugt werden.

Im Falle dieser Arbeit, wird ein besonderer Aspekt in der 3D-Echtzeitberechnung
beleuchtet , das Portal Rendering bzw. Portal Culling. Innerhalb der Arbeit
werden die verschiedenen Anwendungsmöglichkeiten dieser Technik dargelegt und
mit Beispielen fundiert.

Als Basis dieser Arbeit dient das jVR-Framework, welches um die Funktionen des
Portal Cullings erweitert wird. Dies ermöglicht den Aufbau von Szenen, die sich
in weitere Szenen unterteilen lassen.

\section{Aufbau der Arbeit}

Zu Beginn der Arbeit werden mehrere Konzepte in Bezug auf Portal Rendering bzw.
Portal Culling vorgestellt. Dies soll einen Überblick geben in welchen Rahmen
diese Technik eingesetzt wird und welchen Nutzen in 3D-Anwendungen daraus
gezogen werden können. Zusätzlich werden Beispiele aufgezählt in denen sich die
Anwendung von Portal Rendering besonders lohnt.

Ein weiterer Teil der Arbeit betrifft das jVR-Framework. In diesem Abschnitt
werden die verschiedenen Konzepte, die in diesem Framework ihre Anwendung finden
genauer beleuchtet. Zudem wird darauf eingegangen wie Anwendungen im
jVR-Framework umgesetzt werden. Die Standards die in dem Framework verwendet
werden, werden ebenfalls behandelt.

Der nächste Abschnitt befasst sich mit dem Entwurf des Portal Cullings. Hier
werden die verschiedenen Portaltypen vorgestellt, die in der Implementierung
erstellt werden sollen. Es werden außerdem die verschiedenen Vorgehensweisen
betrachtet, mit denen die Portale erstellt werden.

Das Kapitel der Praktischen Umsetzung befasst sich mit der eigentlichen
Implementierung. Dort werden Klassen und Funktionalitäten näher erklärt. Es
werden die verschiedenen Techniken betrachtet mit denen die Portale realisiert
wurden. Außerdem werden die verwendeten DesignPattern begründet und
dargelegt.

Das vorletzte Kapitel gibt einen Überblick über die Ergebnisse. Es
wird untersucht, in wie fern die Anforderungen der Arbeit erfüllt wurden. Zudem
werden die Ergebnisse kritisch betrachtet und analysiert.

Im letzten Kapitel wird eine Zusammenfassung gegeben, die nocheinmal die Arbeit
in kurzen Worten wiedergibt. Zuletzt wird ein Ausblick gemacht, wie Ergebnisse
verbessert bzw. erweitert werden können.

\chapter{Stand der Technik}

Das folgende Kapitel soll einen Überblick darüber geben, was Portal Culling ist
und wie es in der 3D-Echtzeitgrafik Anwendung findet. Zudem werden andere
Algorithmen vorgestellt, die die Performanz in 3D-Anwendungen steigern. Im
weiteren Verlauf wird das jVR-Framework genauer betrachtet wichtige Funltionen
diskutiert. Die Standards, die in dem Framework verwendet werden, werden
ebenfalls vorgestellt.

\section{Portal Culling}

Portal Culling ist ein Verfahren, um die Leistung in 3D-Echtzeitberechnungen zu
steigern. In diesem Verfahren werden bestimmte Objekte ausgewählt, die
letztendlich gerendert werden und damit für den Betrachter sichtbar werden.

Beim Portal Culling stellt man sich einen Bereich vor, der aus mehreren Zellen
besteht. Eine Zelle setzt sich aus mehreren Wänden zusammen und kann an weitere
Zellen angrenzen. Diese Zellen verbinden einander durch Türen bzw. Fenster, so
genannte Portale. Portale sind in der Regel durchsichtig und können in den
meisten Fällen auch betreten werden.

Dadurch lassen sich große Szenen erstellen, die innerhalb eines Gebäudes oder
eines Tunnelsystems spielen. Die Komplexität lässt sich beliebig skalieren, da
sich beliebig viele Zellen miteinander verbinden lassen.

Der Sinn des Portal Cullings besteht darin, Objekte innerhalb der
verschiedenen Zellen zu rendern bzw. nicht zu rendern je nach Sichtbarkeit des
jeweiligen Objektes. Wird z.B. ein Objekt durch eine Wand verdeckt, besteht
keine Notwendigkeit das Objekt zu rendern und es wird zudem Rechenzeit
gespart. Durch den Einsatz dieses Verfahrens können bis zu 50 Prozent der
benötigten Ressourcen zum Rendern eingespart werden.

% \begin{figure}[h]
% \begin{center}
% \includegraphics[height=50mm]{img/portalsplate1}
% \end{center}
% \caption{Test 1234}
% \end{figure}

Zellen können auch ganz vom Renderprozess ausgeschlossen werden. Dies geschieht,
wenn das Portal, das zur nächsten Zelle führt außerhalb des Sichtbereiches
liegt. Dadurch wird die komplette Zelle und alle enthaltenen Objekte nicht
gerendert.

Der erste Portal Culling Algorithmus wurde 1990 von Airey vorgestellt. Im
späteren Verlauf wurden von \cite{visComp} bzw. \cite{visAlgo}
verbesserte komplexere und vor allem effizientere Algorithmen zum Thema Portal
Culling entwickelt.

All diese Algorithmen gleichen sich in der Annahme, dass die Wände als
verdeckendes Element für Szenen dienen, die innerhalb eines Raumes stattfinden.
Des Weiteren wird durch jedes Portal ein View Frustum Culling (siehe Frustum
Culling) durchgeführt. Das eigentliche Frustum wird durch das Portal auf dessen
Größe reduziert, somit werden alle Objekte und auch Portale außerhalb des
Sichtbereichs vom Rendern ausgeschlossen. Diese Vorgehensweise kann rekursiv
fortgesetzt werden, falls sich innerhalb des betrachteten Raumes ein weiteres
Portal befindet.

Um Portal Culling zu betreiben muss ein gewisser Aufwand von Vorberechnungen
erledigt werden. Nach \cite{portals} muss eine Szene in folgenden Schritten
gerendert werden um ein optimales Ergebnis zu erreichen.

\begin{enumerate}
  \item Als erstes muss die Zelle \emph{V} bestimmt werden, in der sich der
  Viewer bzw. die Betrachterkamera befindet.
  \item Des weiteren soll eine 2-dimensionale \emph{Bounding Box} \emph{P}
  erzeugt werden, die in Maßen dem rechteckigen Bildschirm entspricht.
  \item Danach wird die Geometrie der Zelle \emph{V} gerendert. Dabei wird View
  Frustum Culling angewendet. Das Frustum, das vom Betrachter ausgeht wird aus
  dem Rechteck \emph{P} gebildet, in diesem Falle ist das der ganze Bildschirm.
  \item Alle Portale der benachbarten Zellen von \emph{V} werden rekursiv
  durchlaufen. Für jedes Portal der aktuellen Zelle, in der sich der Betrachter
  befindet wird das Portal auf den Bildschirm projeziert. Daraufhin wird eine
  2-dimensionale, an den Achsen ausgerichtete \emph{Bounding Box} (BB)
  erzeugt. Dann wird die Schnittmenge zwischen der BB und \emph{P} ermittelt.
  \item Für jede Schnittmenge gilt: Ist die Schnittmenge leer, ist die
  benachbarte Zelle, die über das Portal verbunden ist aus dem aktuellen
  Betrachterstandpunkt nicht sichtbar. Daher kann die Zelle aus dem
  Rendervorgang ausgeschlossen werden. Ist die Schnittmenge wiederum nicht leer,
  wird der Inhalt der Zelle an Hand des Frustums, welches vom Betrachter
  ausgeht und aus der Schnittmenge gecullt.
  \item War die Schnittmenge nicht leer, kann es möglich sein, dass die Nachbarn
  der benachbarten Zelle ebenfalls sichtbar für den Betrachter sind. Dabei wird
  der dritte Schritt wiederholt, diesmal mit der Schnittmenge als \emph{P}.
  Jedes Objekt, welches gerendert wurde sollte markiert werden, um es kein 2tes
  Mal zu rendern.
\end{enumerate}

\subsection{Portale als Spiegel}

Ein weiterer Verwendungszweck von Portal Culling ist die Erzeugung von Spiegeln.
Spiegel reflektieren den Sichtbereich des Betrachters. Wenn man aus Sicht des
Spiegels in die Szene schaut und diese nochmals spiegelt, erhält man das Bild
welches der Betrachter bei Blick in den Spiegel erhält. 

Man nimmt den Spiegel in diesem Fall als Spiegelebene und erstellt an der
gespiegelten Position des Betrachters eine Kamera. Diese rendert dann die Szene
durch den Spiegel. Danach wird die gerenderte Szene aus Sicht der Spiegelkamera
auf den Spiegel projeziert.

Dies wird dadurch erreicht, indem man die Kamera $C$ in das
Koordinatensystem der Spiegelebene überträgt. Besitzt die Ebene eine
Transformation $M$, wird die Kamera über $M^{-1}$ das
Objektkoordinatensystem der Ebene transformiert. Mit der Skalierung
$S(1,1,-1)$ wird die Kamera gespiegelt (vergleiche auch \cite[S. 33]{jvrMast}).

\begin{equation}
{C}_{mirror} := M * S * M^{-1} * C
\end{equation}

Bei diesem Verfahren werden aber auch Geometrien dargestellt, die eigentlich
hinter dem Spiegel liegen. Um dies vorzubeugen wird der Gebrauch von
Clippingebenen gemacht. Die Clippingebene verwirft alle Fragmente die sich
hinter Spiegel befinden und daher nicht gespiegelt werden können.

Spiegel werden häufig in Szenen eingesetzt, die eingerichtete Wohnungen oder
Räumlich-keiten repräsentieren sollen. Dadurch bekommt der Betrachter einen
Eindruck davon, wie der Spiegel innerhalb des Raumes wirkt.

In Computerspielen dienen Spiegel oft als Stilmittel, um z.B. die eigene
Spielfigur betrachten zu können (z.b. First-Person Shooter). Oder um
atmosphärische Spannung zu erzeugen, wenn z.B. eine geistähnliche Gestalt im
Spiegel erscheint.

Mit Hilfe von Shadern kann hinzukommend noch eine glasähnliche Oberfläche
simuliert werden, was den Eindruck eines Spiegels nochmals verstärkt. Außerdem
lassen sich zudem noch Materialschäden oder Abnutzungserscheinungen hinzufügen
um den Eindruck von Verschleiß zu erwecken.

\subsection{Portale als Teleporter}

Zudem lassen sich Portale dazu nutzen sich innerhalb der Umgebung zu
teleportieren. Das bedeutet, dass man sich von einem Ort zu einem anderen
Ort in kürzester Zeit bewegt, obwohl diese sehr weit von einander entfernt
liegen. Dazu werden 2 oder auch nur ein Portal erzeugt und der Ausgang des einen
Portals stellt den Frustum des anderen Portals dar. Dabei wird eine Kamera
benutzt die genau den entgegengesetzten Sichtbereich des ersten Portals rendert,
wobei aber der Sichtbereich auf das zweite Portal gerendert wird. Bei
Durchtreten der beiden Portale wird die Position des Betrachters dementsprechend
verändert.

Möglich ist es zudem ein Portal als Einbahnstraße zu benutzen, wodurch der
Betrachter nicht mehr in der Lage ist an die Stelle des durchtretenden Portals
ohne Probleme zurückzukehren. Dies bedeutet, dass der Betrachter zwar von
Stelle 1 nach Stelle 2 teleportiert wird, wenn er das Portal durchtritt, aber
nicht mehr von Stelle 2 an die Stelle 1 gelangen kann, da sich dort kein Portal
befindet. Man erreicht dies, indem man die Szene an der Stelle durch das
Portal rendert, an der der Spieler austreten soll. Daher wird an diese
Stelle mit dem Verhältnis zur Betrachterkamera die Szene gerendert und auf
der Portaloberfläche gerendert. Diese Herangehensweise lässt sich oft in
Computerspielen wiederfinden, um z.B. ein Labyrinth zu erzeugen oder den Spieler
kurz zu desorientieren.

\subsection{Frustum Culling}

Frustum Culling ist ein Verfahren in der Computergrafik um die Performanz beim
Rendern von Szenen zu steigern. Bei diesem Verfahren werden unnötige Objekte vom
Renderprozess ausgeschlossen.

Beim Frustum Culling wird ein Sichtbereich aufgespannt, das so genannte Frustum.
Es wird überprüft welche Objekte sich innerhalb des Frustums befinden und welche
nicht. Befindet sich ein Objekt innerhalb, wird es gerendert. Ist dies nicht der
Fall wird es verworfen.

Es werden geometrische Primitive um ein Objekt gezeichnet, um festzustellen, ob
es sich innerhalb des Frustums befindet. Diese Primitive nennt man Bounding
Boxes (BB). Da die BB größer ist als das eigentliche Objekt, kann es dazu
führen, dass ein Objekt zwar nicht mehr zu sehen ist aber noch mit seiner BB den
Frustum berührt und daher noch gerendert wird.

\subsection{Occlusion Culling}

Ein weiteres Verfahren um die Performanz zu optimieren ist das Occlusin Culling.
Ähnlich dem Frustum Culling werden Objekte bestimmt, die nicht in den
Renderprozess gehören, da nicht sichtbar.

Anders als beim Frustum Culling werden Objekte nicht außerhalb des Frustums
verworfen. Beim Occlusion Culling wird bestimmt, ob ein Objekt von einem anderen
Objekt verdeckt und damit für den Betrachter nicht sichtbar ist. Diese
verdeckenden Objekte werden als Occluder bezeichnet.

Es gibt 2 Ansätze Occlusion Culling durchzuführen. Einmal gibt es den
punktbasierten Ansatz. In diesem wird die Szene aus Sicht des Betrachters bzw.
eines festen Punktes betrachtet. Dabei werden alle Objekte betrachtet und je
nach Sichtbarkeit zum Renderprozess hinzugefügt oder ausgeschlossen. Diese
Vorgehensweise muss bei Bewegung des Punktes wiederholt werden.

Der zweite Methode stellt der zellenbasierte Ansatz dar. Bei dieser
Vorgehensweise wird eine Zelle von einer bestimmten Größe definiert. Danach
werden alle Objekte bestimmt die innerhalb dieser Zelle für den Betrachter nicht
sichtbar sind, da verdeckt. Diese Methode bietet den Vorteil den Vorgang nur
dann zu wiederholen, falls man den Bereich der Zelle verlässt.

\section{Techniken des jVR-Framework}
% TODO: besser formulieren, jVR wurde nicht von mir ausgewählt, ist Teil der
% Arbeit
Wie schon vorangehend erwähnt, spielt der Einsatz des jVR-Frameworks eine
zentrale Rolle dieser Arbeit. Dies lässt sich zum einen dadurch begründen, dass
das Framework schon ein grundlegendes Maß an wichtigen Funktionen bietet, die
nötig sind um 3D-Echtzeit-Anwendungen zu erstellen und zum anderen, da es um
weitere nützliche Funktionen erweitert werden kann um Performance bzw.
Produktivität zu steigern.

Das jVR-Framework ist im Rahmen einer Masterarbeit entstanden. Das Framework
soll den Studenten eine Plattform liefern mit denen sie komplexe
3D-Programme erzeugen können. Außerdem soll es den Studenten ein besseres
Verständnis über die Funktionen von 3D-Engines vermitteln.

\subsection{Szenegraph}

Um die Objekte innerhalb einer 2D bzw. 3D-Szene verwalten zu können, wird in dem
jVR-Framework Verwendung von einem Szenegraphen gemacht. Ein Szenegraph
ist eine baumförmige Datenstruktur. Objekte in einer Szene können zum einen
geometrische Objekte oder Lichtquellen sein. Jedes Objekt in der Szene besitzt
eine eigene Transformation. Es besteht außerdem die Möglichkeit Objekte einer
Szene zu animieren. Dies geschieht, indem man die Transformationen in zeitlichen
Intervallen manipuliert.

Über einen Szenengraph lassen sich außerdem Objekte animieren, die in
hierarchischer Beziehung zueinander stehen. Werden mehrere Objekte in einem
Gruppenknoten zusammengefasst entsteht eine hierarchische Beziehung. Dabei kann
der ganze Gruppenknoten transformiert werden, mitsamt den enthaltenen Objekten
oder aber jeder einzelne Objektknoten innerhalb des Gruppenknoten.

\subsubsection{Szeneknoten}

Das jVR-Framework unterscheidet zwischen mehreren Knotentypen. Es gibt zum einen
den schon erwähnten \texttt{GroupNode}. Dieser Knoten kann als einziger
Kinderkoten enthalten. Zusätzlich gibt es noch den \texttt{LightNode}. Dieser
repräsentiert die verschiedenen Lichtquellen und wird von 3 Standardlichtquwllwn
abgeleitet. Diese 3 Lichtquellen sind \texttt{PointLightNode},
\texttt{DirectionalLightNode} und \texttt{SpotLightNode}.

Die \texttt{PointLightNode} strahlt Licht in alle Richtungen gleichzeitig.
Anders hingegen die \texttt{SpotLightNode}, die Licht nur innerhalb eines
gerichteten Kegels ausstrahlt. Die letzte Lichtquelle, die
\texttt{DirectionalLightNode} strahlt Licht parallel in eine bestimmte Richtung.

Die Position des Betrachters wird in der Szene über eine Kamera dargestellt. Der
passende Knotentyp ist die \texttt{CameraNode}. Hinzu kommt die
\texttt{VRCameraNode}, die für den Einsatz in VR-Systemen gedacht ist.

Um geometrische Objekte in einer Szene abzubilden, wird die \texttt{ShapeNode}
genutzt. Einer \texttt{ShapeNode} lassen sich eine Geometrie und ein passendes
Material zuordnen. \texttt{ShapeNodes} lassen sich im Szenegraphen
wiederverwenden. Dazu wird die \texttt{ShapeNode} an verschiedene
\texttt{GroupNodes} gehängt, die unterschiedlich transformiert werden.

Der letzte Knotentyp, über den das jVR-Framework verfügt, ist die
\texttt{ClipPlaneNode}. Sie dient dazu, eine Szene in 2 Halbräume zu unterteilen
und einen der beiden Räume auszublenden.

\subsubsection{Transformationen}

Das jVR-Framework verfügt über mehrere Möglichkeiten Knoten innerhalb einer
Szene zu transformieren. Es werden 5 Arten der Transformation unterschieden,
vergleiche dazu auch \cite{compOL}.

\begin{align}
\textbf{Transformation:} &
\begin{pmatrix}
\phantom{-\sin \alpha}&\phantom{-\sin \alpha}&\phantom{-\sin
\alpha}&\phantom{-\sin \alpha}\\[-2.5ex] 
1 & 0 & 0 & {t}_{x} \\
0 & 1 & 0 & {t}_{y} \\
0 & 0 & 1 & {t}_{z} \\
0 & 0 & 0 & 1
\end{pmatrix} \\
\textbf{Skalierung:} &
\begin{pmatrix}
\phantom{-\sin \alpha}&\phantom{-\sin \alpha}&\phantom{-\sin
\alpha}&\phantom{-\sin \alpha}\\[-2.5ex]
{s}_{x} & 0 & 0 & 0 \\
0 & {s}_{y} & 0 & 0 \\
0 & 0 & {s}_{z} & 0 \\
0 & 0 & 0 & 0
\end{pmatrix}
\end{align}
\begin{align}
\textbf{Rotation um X-Achse:} &
\begin{pmatrix}
\phantom{-\sin \alpha}&\phantom{-\sin \alpha}&\phantom{-\sin
\alpha}&\phantom{-\sin \alpha}\\[-2.5ex] 
1 & 0 & 0 & 0 \\
0 & \cos \alpha & -\sin \alpha & 0 \\
0 & \sin \alpha & \cos \alpha & 0 \\
0 & 0 & 0 & 1 
\end{pmatrix} \\
\textbf{Rotation um Y-Achse:} &
\begin{pmatrix}
\phantom{-\sin \alpha}&\phantom{-\sin \alpha}&\phantom{-\sin
\alpha}&\phantom{-\sin \alpha}\\[-2.5ex] 
\cos \beta & 0 & \sin \beta & 0 \\
0 & 1 & 0 & 0 \\
-\sin \beta & 0 & \cos \beta & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}\\
\textbf{Rotation um Z-Achse:} &
\begin{pmatrix}
\phantom{-\sin \alpha}&\phantom{-\sin \alpha}&\phantom{-\sin
\alpha}&\phantom{-\sin \alpha}\\[-2.5ex] 
\cos \gamma & -\sin \gamma & 0 & 0 \\
\sin \gamma & \cos \gamma & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}
\end{align}

\subsubsection{Traversierung}

Um eine baumförmige Datenstruktur wie den Szenegraph zu durchlaufen wird
Gebrauch von der Traversierung gemacht. Dabei werden die einzelnen Knoten der
Szene verarbeitet. Traversierung macht vor allem dann Sinn, wenn z.B.
Szeneknoten mit bestimmten Eigenschaften gesucht werden müssen. 

Zum Rendern muss der Szenegraph ebenfalls durchlaufen werden. Angefangen wird
dabei beim Wurzelknoten. Jeder Szeneknoten der an der Wurzel hängt wird in
Welttransformation überführt. Dies geschieht, indem alle Transformationen vom
Wurzelknoten bis zum Szeneknoten multipliziert werden.

\subsection{Drawlisten}

In Drawlisten werden die verschiedenen Szeneknoten mit ihren Weltransformationen
gesammelt und nacheinander gerendert. Eine Drawliste kann zudem beliebig oft
verwendet werden. Das bietet den Vorteil, den Szenegraph nicht nochmals zu
traversieren, falls ein Objekt mehrmals gerendert werden muss.

Nachdem die Drawlisten erstellt wurden, werden sie in der Pipeline gespeichert
und ja nach Bedarf genutzt werden. Das heißt, dass Objekte nach bestimmten
Materialeigenschaften gefiltert und gerendert werden können, siehe \cite[S.
47-48]{jvrMast}.

\subsection{Pipelinekonzept}

Die Pipeline im jVR-Framework macht es möglich einzelne Szeneobjekte getrennt
voneinander zu rendern. Man kann zudem verschiedene Materialien zu
Materialklassen zusammenfassen. Materialklassen können wiederum aus mehreren
Shaderkontexten bestehen und somit aus mehreren Shaderprogrammen. 

Shaderkontexte werden größtenteils dafür verwendet Objekte richtig zu
beleuchten. Erst wird mit einem Shaderprogramm die Umgebungsbeleuchtung
(Ambient-Pass) für das Objekt gerendert. Anschließend darauf werden die Objekte
für jede Lichtquelle mit dem Shaderprogramm mit der eigentlichen Beleuchtung
gerendert (Lighting-Pass). Die zuvor erzeugten Bildinformationen werden mittels
des Blending-Verfahrens aufaddiert, (vergleiche auch \cite[S. 46]{jvrMast}).

\subsection{FrameBuffer Object}

Um beim jVR-Framework mehrere Szenen in einem Bild zusammenlegen zu können, wird
das \emph{FrameBuffer Object}, kurz FBO verwendet. Mit dem \emph{FrameBuffer
Object} kann direkt in eine Textur gerendert werden. Das bietet den Vorteil das
eine Szene erst erzeugt wird und in die Textur geschrieben wird und danach noch
mit Grafikoperationen verändert wird.

Eine weitere Möglichkeit ist die Szene in ein \emph{FrameBuffer Object} zu
rendern und die erzeugte Textur auf eine vorhandene Geometrie zu projezieren.
Als Beispiel lässt sich der Spiegel nennen. Wird die Szene gespiegelt gerendert
und danach auf eine Ebene projeziert erhält man eine spiegelnde Oberfläche.

Das \emph{FrameBuffer Object} findet vor allem dann Anwendung, wenn eine Szene
nachträglich verändert werden soll. Dieses Verfahren wird auch
\emph{Post-Processing} genannt. Diese Nachbearbeitung dient dazu ein optischen
Effekt zu erzeugen, der dem Anwender ein realeres Bild vermitteln soll.

Depth-of-Field ist ein solcher Effekt, der mit der Hilfe des
\emph{FrameBuffer Objects} realisiert wird. Dabei wird ein Z-Buffer-Test von
der Szene gemacht und in die Textur geschrieben. Die erhaltenen Informationen
stellen den Abstand der Pixel zum Betrachter dar. Je weiter ein Pixel vom
Betrachter weg steht, desto stärker wird dieser weichgezeichnet.

\subsection{Virtual Reality}

Zusätzlich bietet das jVR-Framework noch die Möglichkeit eine Art virtueller
Realität zu erzeugen. Dabei werden stereoskopische Bilder erzeugt, die durch
eine entsprechende Brille einen räumlichen Effekt simulieren. Dies wird
durch die Funktion des Multithreading geschafft.

Multithreading bedeutet mehrere Prozesse bzw. Aufgaben von der Engine
gleichzeitig durchführen zu lassen. Im Falle vom jVR-Framework lassen sich
mehrere Szenen gleichzeitig rendern. Es lassen sich zudem mehrere Anzeigegeräte
ansteuern. Dabei werden die zu erzeugenden Teilbilder parallel gerendert.

Um stereoskopische Bilder zu erzeugen werden 2 voneinander versetzte Kameras
aufgestellt und die Szene simultan aus Sicht der beiden Kameras gerendert. Bei
der Darstellung werden die 2 erzeugten Bilder übereinandergelegt. Mit der schon
erwähnten Brille werden die Bilder aus Sicht des Betrachters zusammengefügt. Es
entsteht ein 3dimensionaler Effekt.

Ein weiterer Teil der virtuellen Realität ist die Funktion des Headtrackings.
Dabei werden die Bewegung des Kopfes vom Betrachter aufgezeichnet und vom
Programm verarbeitet. Je nach Neigung bzw. Drehung des Kopfes wird die
betrachtete Szene angepasst, was dem Betrachter die Illusion vermittelt sich
tatsächlich in der Szene zu bewegen.

\section{Verwendete Standards im jVR-Framework}

\subsection{OpenGL}

Für die 3D-Beschleunigung wird die Grafikbibliothek OpenGL verwendet. \emph{Open
Graphics Library}, kurz OpenGL ist ein offener Grafikstandard, der zur
2D- und 3D-Echtzeit-berechnung verwendet wird.

OpenGL geht aus der von Silicon Graphics Inc. (SGI) entwickelten IRIS GL API
hervor. Ab 1992 wurde OpenGL vom OpenGL ARB (Architecture Review Board)
beaufsichtigt und weiterentwickelt. Heute ist die Khronos Group für die
Weiterentwicklung der API zuständig. Die Khronos Group zählt weit über 100
Mitglieder, darunter AMD, Intel und NVIDIA.

OpenGL wird von nahezu jeder Grafikkarte unterstützt, die richtigen Treiber
vorausgesetzt. Die Besonderheit von OpenGL ist im Vergleich zu Direct3D seine
Plattformunabhängigkeit. Ebenso wie Java lässt sich eine Implementation von
OpenGL auf jedem gängigen Betriebssystem, wie Windows, Mac OS X und Linux
finden. Dies erleichtert Programmierern Anwendungen zu erstellen und in
verschiedenen Umgebungen zu testen.

OpenGL kann als Zustandsautomat verstanden werden. Wird z.B. eine Farbe oder
eine Transformation festgelegt, werden alle daraufhin gezeichneten Objekte mit
der gewählten Farbe koloriert und der festgelegten Transformation transformiert.

OpenGL besaß bis zur Version 1.5 eine Fixed Function Pipeline mit der die
Objekte gerendert werden mussten. Die Fixed Function Pipeline war für die
Beleuchtung und Transformation der Vertices verantwortlich. Erst mit der Version
2.0 ließ sich die Pipeline über die \emph{OpenGL Shading Language} zum Teil
programmieren.

Seit OpenGL 3.0 wurde damit begonnen die meisten Befehle der Fixed Function
Pipeline aus der Spezifikation zu entfernen. Mit der Veröffentlichung von OpenGL
3.1 ist die Fixed Function Pipeline komplett aus der Spezifikation verschwunden.
Die Befehle lassen sich aber weiterhin über OpenGL-Extensions nutzen.

%TODO Abbildung Pipeline OpenGL

Die Abbildung zeigt die OpenGL Pipeline. Zuerst werden die Attribute der
Vertices an den Vertex-Shader übergeben. Dieser reicht die verarbeiteten Daten
weiter an den Geometry-Shader. Nachdem die Geometrien erzeugt bzw. verarbeitet
wurden wird das Clipping vorgenommen. Nachdem dieser Schritt beendet ist, wird
die erhaltene Szene auf den Bildschirm gemappt. Das bedeutet die Szene wird aus
Sicht der Kamera gerendert und danach über den Rasterizer in die festgelegte
Auflösung überführt.

Die rasterisierte Szene wird dem Fragment-Shader übergeben, der weitere
Verarbeitungsschritte vornimmt. Zum Beispiel werden die Sichtbarkeit der
einzelnen Pixel getestet (Z-Buffer). Zum Schluss werden alle erzeugten Fragmente
zusammengelegt und den FrameBuffer geschrieben. Der FrameBuffer wird entweder
von der Pipeline als Textur weiterverwendet oder in einer Fensterumgebung auf
dem Bildschirm ausgegeben.

\emph{OpenGL Embedded Systems}, kurz OpenGL ES ist eine spezielle
Zusammenstellung der OpenGL API für mobile Geräte. Dadurch lassen sich
Anwendungen auf mobilen Geräten realisieren. Durch die begrenzten Kapazitäten,
wie Arbeitsspeicher und Rechenleistung der mobilen Hardware, ist diese Version
der OpenGL API um einige Funktionen erleichtert worden.

%TODO besser formulieren
Ins jVR-Framework wurden viele häufig benutzten Grundfunktionen implementiert.
Dies dient dazu um die Produktivität zu steigern. Dadurch kommt der Entwickler
so gut wie nie in Kontakt mit reinen OpenGL-Befehlen.

\subsection{GLSL}

Eine weitere Besonderheit des jVR-Frameworks bietet die Unterstützung von
Shadern, die in der \emph{OpenGL Shader Language}, kurz GLSL geschrieben werden.
Shader sind kleine Programme die direkt auf der GPU ausgführt werden. Sie
ersetzen zudem die Fixed Function Pipeline von OpenGL.

GLSL besitzt eine C-ähnliche Syntax und besitzt zudem noch zusätzliche
Datentypen wie Vektoren und Matrizen. Vor der Verwendung müssen die Shader
compiliert werden. Dies bietet den Grafikkartenherstellern den Vorteil, den
Compiler für die eigene Hardware zu optimieren.

GLSL bietet außerdem die Möglichkeit ein besseres Beleuchtungsmodell zu
verwenden, als das von der Fixed Function Pipeline bereitgestellte. Seit Version
3.1 von OpenGL sind Fragment- und Vertex-Shader ein wichtiger Bestandteil der
Spezifikation und müssen zwingend implementiert werden. Durch Verwendung dieser
Shader lassen sich Oberflächeneffekte oder auch Effekte, wie Motion Blur und
Depth of Field erzeugen.

Mit OpenGL 3.2 kommen die Geometry-Shader zur Kernspezifikation hinzu. Diese
Shader ermöglichen es Geometrien direkt auf der Grafikkarte zu erzeugen und zu
verarbeiten. OpenGL 4.0 erweitert die Spezifikation um den Gebrauch von
Tesselation-Shadern. Diese Shader können dazu verwendet vorhandene Polygone in
weitere kleinere Polygone aufzuteilen und so die Oberfläche eines geometrischen
Objekts zu verbessern.

\subsection{JOGL}

Im jVR-Framework wird die \emph{Java Bindings for OpenGL} (JOGL) verwendet, um
OpenGL mit Java nutzen zu können. JOGL stellt eine Wrapperbibliothek dar, die die
OpenGL-Befehle über Javafunktionen ansteuern lässt.

Auf die native OpenGL API wird mit Hilfe des \emph{Java Native Interface} (JNI)
zugegriffen. Der native C-Code ist auf allen großen Systemen wie Windows, Mac OS
X und Linux verfügbar.

JOGL wurde von der Sun Microsystems Game Technology Group entwickelt. Es steht
als Open Source zur Verfügung und wird unter der BSD Lizenz angeboten. Es stellt
zudem eine Referenzimplementierung der Java Specification Request (JSR-231) dar.
JOGL wird möglicherweise in einer späteren Java-Version zu den
Standarbibliotheken mit aufgenommen.

In der Version 2 von JOGL wurden die GLProfile eingeführt. Diese ermöglichen es
dem Entwickler sich für ein OpenGL-Version zu entscheiden. Dadurch kann
bestimmt werden auf welchen Plattformen die erstellten Anwendungen laufen
und es werden zudem Kompatibilitätsprobleme vermieden.

Das jVR-Framework benutzt in diesem Fall das GL2GL3-Profil. Es vereint alle
Funktionen der Profile GL2 und GL3. Das Profil hat die Eigenschaft GL3 konform
zu sein. Es können daher GL3-Anwendungen entwickelt werden, die aber auch auf
Systemen lauffähig sind, die nur OpenGL 2 unterstützen.

% \begin{figure}[ht]
% 	\includegraphics[width=\linewidth]{img/glprofiles.png}
% 	\caption{Test 123}
% \end{figure}

\subsection{jBullet}

Um physikalische Effekte in dem jVR-Framework zu realisieren wird Gebrauch von
jBullet gemacht. jBullet ist eine Bibliothek, die die Möglichkeit bietet
Kollisionen zwischen Objekten zu erkennen und geometrische Objekte mit
physikalischen Eigenschaften zu versehen.

Es kann ein physikalische Umgebung beschrieben werden, die betimmten
physikalische Gesetzen folgt. Zum Beispiel kann die Stärke der Gravitation und
das Gewicht der Körper festgelegt werden. Dadurch lassen sich Szenen entwickeln,
die entweder eine realistische Physik bieten, wie sie auf der Erde vorhanden ist
oder aber mondähnliche Verhältnisse geschaffen werden, in denen die Körper in
trägerer Art und Weise durch die Gegend bewegt werden können.

Die Engine bietet zudem eine Kollisionserkennung. Mit ihrer Hilfe lassen sich
Situationen simulieren, in denen mehrere Objekte miteinander kollidieren. Die
Objekte müssen sich je nach Gewicht und Beschaffenheit korrekt verhalten. Diese
komplexen Berechnungen übernimmt die jBullet-Engine.

Die jBullet-Engine ist eine Portierung der Bullet-Engine. Bullet wurde in C++
entwickelt. Die Bullet-Engine ist eine weit verbreitete Physik-Engine, die
selbst in vielen kommerziellen Projekten Anwendung findet. Sie wurde zum
Beispiel in Filmen wie 2012, Shrek 4 und A-Team verwendet.

Viele der bekannten 3D-Authoring Programme lassen sich mit einem PlugIn
erweitern, das alle Eigenschaften der Bullet-Engine integriert. Damit lassen
sich auch in diesen Programmen physikalisch korrekte Szenen simulieren.

\subsection{COLLADA}

Ein wichtiger Bestandteil des jVR-Frameworks stellt die Benutzung des
3D-Austausch-formats COLLADA dar. Dieses Format ist basiert auf der
Auszeichnungssprache XML. Es dient dazu wichtige Daten zwischen verschiedenen
3D-Programmen zu verwalten. Weiterhin ist es als offener Standard definiert.

Eine COLLADA-Datei setzt sich aus mehreren Bereichen zusammen. Zum einen werden
die Geometrien beschrieben und zum anderen werden die verwendeten Texturen über
Pfade angesteuert. Weitere Bereiche sind Lichter, Materialien und Kameras.

Seit der Version 1.4 können auch physikalische Eigenschaften in COLLADA-Dateien
hinterlegt werden. Dabei können Oberflächeneigenschaften wie Reibung definiert
werden.

Das COLLADA-Format wird von den meisten 3D-Authoring-Programmen
unterstützt, darunter Blender, Autodesk Maya und Autodesk 3D Studio Max.
Außerdem kommt es in Google SketchUp und Google Earth zur Anwendung, was zu
einem Großteil der Verbreitung beigetragen hat.

\chapter{Entwurf des Portal Cullings}

Alle zuvor diskutierten Themen stellen die Schwerpunkte der Arbeit dar. Die
Integration einer Portalfunktion in das jVR-Framework ist dabei als
Hauptschwerpunkt zu sehen.

Der Entwurf soll noch einmal die wichtigsten Konzepte wiedergeben. Zum einen
werden die verschiedenen Arten von Portalen beschrieben und erläutert. Hinzu
kommen die Verwendungen von virtuellen Kameras und deren Rolle beim erstellen
von Portalen.

\section{Portaltypen}

In Szenen gibt es mehrere Möglichkeiten in andere Räume zu schauen. Zum einen
kann der Betrachter direkt aus einem Fenster gucken oder sieht direkt durch die
offene Tür in den nächsten Raum. Zudem lässt sich durch den Blick in den Spiegel
der hintere Bereich des Raumes betrachten. All diese Bestandteile eines normalen
Raumes haben die Gemeinsamkeit, sich als Portal wiedergeben zu lassen.

Jeder dieser Portaltypen erhält seine eigene virtuelle Kamera, die die Szene aus
verschiedenen Blickwinkeln rendert und auf das jeweilige Portal projeziert wird.
Damit ist jeder von diesen Typen von der Klasse Portal abgeleitet.

\subsection{Türen}

Türen bieten die Besonderheit durchtreten werden zu können. Somit bieten sich
Türen als einziger Portaltyp an mehrere Räume miteinander zu verbinden. Türen
werden entweder direkt in einer Szene platziert und werden mit einem sich von
der Umgebung abhebenden Umriss versehen. Wenn eine Tür in die Szene platziert
wird, wird zudem ein virtuelle Kamera mitinstanziert um die Szene durch diese
gerendert werden kann.

Türen eignen sich auch um Culling durchzuführen. Wenn der Betrachter durch die
sieht, wird das Frustum auf den sichtbaren Umriss der Tür reduziert. Alle
Gegenstände, dessen Bounding Box innerhalb des View Frustums liegen werden
mitgerendert.

Da die Tür als Trenner der Räume (auch als Zellen bezeichnet) dient, kann
festgelegt werden welche 2 Räume miteinander verbunden sind. Dabei kann
ermittelt werden, welche Räume überhaupt nötig zum Rendern sind.

\subsection{Spiegel}

Ein besonderer Portaltyp ist der Spiegel. Er kann genau wie das Fenster nicht
durchtreten werden. Außerdem dient er nicht dazu Räum miteinander zu verbinden.
Der Spiegel bietet die Funktion den Sichtbereich des jeweiligen Betrachters aus
Sicht des Spiegel zu reflektieren und zu projezieren. 

Dabei dient die Spiegeloberfläche als Reflexionsebene an der der Standpunkt des
Betrachters gespiegelt wird und in Form einer virtuellen Kamera wiedergegeben
wird. Diese virtuelle Kamera rendert mit negiertem Seitenverhältnis die Szene
aus Sicht der Spiegelfläche und negativer Skalierung.

\subsection{Teleporter}

Der Teleporter ist eine Abwandlung der Tür. Im Gegensatz zur Tür, verbindet der
Teleporter keine Räume miteinander. Der Teleporter dient als eine Art
Schnellreisesystem. Er verbindet 2 Türen miteinander, die im Raum sehr weit weg
voneinander entfernt liegen. Sie werden über den \texttt{PortalConnector}
verbunden.

Wie der Spiegel erhält jeder Teleporter eine virtuelle Kamera. Diese Kamera
werden relativ zur Position der Kamera des Betrachters zum Teleporter auf der
Gegenseite ausgerichtet. Je nach Bewegung des Betrachters bewegt sich auch die
virtuelle Kamera.

Teleporter prüfen, ob eine \texttt{SceneNode} nahe genug steht um teleportiert
zu werden. Ist dies der Fall, wird die \texttt{SceneNode} an die Position des
festgelegten Ausgangs transformiert. Zudem werden die transformierten
\texttt{SceneNodes} nicht in ihrer Orientierung oder Bewegungsrichtung
beeinflusst. Durchtritt ein Betrachter den \texttt{Teleporter} blickt immernoch
in die gleiche Richtung, die ihm die Ausgangsprojektion suggeriert hat.

\section{Virtuelle Kameras}

Die virtuellen Kameras sind ein wichtiger Bestandteil der Portale. Für jede
virtuelle Kamera wird ein \texttt{FrameBufferObjekt} angelegt. Das
\texttt{FrameBufferObject} ist die Repräsentation der Szene aus Sicht der
virtuellen Kamera. Wird die Szene aus Sicht der Betrachterkamera gerendert,
werden alle erzeugten \texttt{FrameBufferObjects} auf das jeweilige Portal
projeziert.

Es gibt 2 Portaltypen, die eine virtuelle Kamera besitzen. Zum einen
\texttt{Teleporter} und zum anderen \texttt{Mirror}. Diese beiden Portaltypen
benötigen die virtuelle Kamera um die Szene aus einer gewissen Position zu
rendern im Anschluss auf dem jeweiligen Typen anzuzeigen.

Beim \texttt{Teleporter} rendert die virtuelle Kamera die Szene aus Sicht des
zugeordneten Ausgangs. Dabei wird die Kamera an die Stelle des Ausgangs
transformiert und bekommt zudem die Kameratransformation des Betrachters mit
übergeben. Dies lässt den Betrachter durch den \texttt{Teleporter} blicken und
bei Änderung des Blickwinkels wird auch der Blickwinkel der virtuellen Kamera
mitverändert.

Ähnlich funktioniert die virtuelle Kamera bei \texttt{Mirror}. Die virtuelle
Kamera ist an der Stelle des Spiegels transformiert. Ebenfalls wird die
Transformation der Betrachterkamera berücksichtigt. Bewegt sich die
Betrachterkamera beispielsweise nach rechts, bewegt sich die virtuelle Kamera
aus Sicht des Betrachters nach links. Da das Seitenverhältnis der virtuellen
Kamera negiert ist, wird Eindruck eines Spiegels erzeugt.

\section{Portallisten}

Um alle Portale innerhalb einer Szene rendern zu können, kommen die Portallisten
zum Einsatz. Wenn ein Portal erzeugt wird, wird es gleich darauf der
\texttt{PortalList} hinzugefügt. Sie ruft für jedes Portal die jeweilige
Render-Funktion auf erzeugt dementsprechend das aktuelle Portal.

Zusätzlich verwalten die Portallisten die hinzugefügten Portale und
aktualisieren die Transformationen der virtuellen Kameras. Dabei wird der
Portalliste die Kameratransformation der Betrachterkamera übergeben und
weiterverarbeitet. Dies bietet den Vorteil nicht jedes Portal einzeln verwalten
zu müssen.

\section{Zellen}

Zellen stellen in sich abgeschlossene Räume dar. Sie sind \texttt{GroupNodes}
und können daher weitere Szeneknoten enthalten. Zellen können über Türen
miteinander verbunden werden. Jede Zelle wird durch 4 Wände definiert, wobei die
Breite, Länge und Höhe einer Zelle variieren kann. Innerhalb einer Zelle
befindet sich zudem \texttt{PointLightNode}, um innerhalb positionierte Objekte
richtig darstellen zu können. Eine Zelle lässt sich farblich anpassen, oder aber
mit einer Textur versehen.

Eine Zelle stellt eine \texttt{GroupNode} dar. Daher lässt sie sich nach
Belieben transformieren, zusammen mit allen enthaltenen Kindknoten. Zellen
können somit beliebig platziert werden und daraufhin mit einer benachbarten
Zelle über eine Tür verbunden werden. Die Information ob 2 Zellen benachbart
sind oder nicht wird in der Tür gespeichert.

\chapter{Praktische Umsetzung}

Dieses Kapitel befasst sich mit der Implementierung der Erweiterung für das
jVR-Framework. Im ersten Teil werden die Portal-Klassen und ihre verschiedenen
Methoden genauer betrachtet. Der zweite Teil stellt den Ansatz vor, mit dem das
Portal Culling umgesetzt wurde.

\section{Paketstruktur}

Die Paketstruktur stellt eine Ergänzung zum jVR-Framework dar und fügt sich in
dessen Struktur ein. Die Pakete unterteilen sich daher logisch und überschneiden
sich nicht mit Paketen von jVR. Das Oberpaket \texttt{portals} enthält alle
weiteren Unterpakete, sowie die verschiedenen Typen von Portalen, die im Laufe
der Implementierung entstanden sind.

Das Paket \texttt{culling} besteht aus den Klassen die für Portal Culling von
Bedeutung sind. Mit ihnen können Zellen erzeugt werden, also Räume die
untereinander verbunden sind. Zudem lassen sich verschiedene Arten von Wänden
erzeugen, über die Klasse \texttt{Wall}.

Das \texttt{tests}-Paket ist mit Anwendungen gefüllt, die das Verhalten der
einzelnen Portaltypen testet und demonstriert. Die Tests sind ein wichtiger
Bestandteil für die Entwicklung der Portale und ihren unterschiedlichen
Implementierungen.

Ein Unterpaket des \texttt{tests}-Pakets stellt das \texttt{physics}-Paket dar.
Die Tests innerhalb des Paketes dienen dazu, das Verhalten zwischen
physikalischen Objekten und den Portalen zu untersuchen. Sie werden mit
Komponenten der jBullet-Bibliothek erweitert um physikalisch korrekte Bewegungen
und Kollisionserkennung zu erzeugen.

Das letzte Paket ist das \texttt{util}-Paket. Es enthält alle Hilfsklassen die
für den Umgang von Portalen nötig sind. Außerdem bietet es eine
Standardtestklasse an, die von allen Testklassen für die Portalfunktionen
abgeleitet wird.

\section{Portalimplementierung}

Da es mehrere Typen von Portalen gibt, bietet es sich an eine Oberklasse zu
implementieren von der alle konkreten Portaltypen abgeleitet werden können.
Das Entwurfsmuster, welches als Grundlage dient ist das sogenannte
Kompositionsmuster. Ähnlich der Szeneknoten im jVR-Framework, die alle von der
Oberklasse \texttt{SceneNode} abgeleitet sind, werden die
Portal-Unterklassen von der Oberklasse \texttt{Portal} abgeleitet. Sie selbst
erbt von der Klasse \texttt{GroupNode} und kann daher mehrere Kindknoten
beinhalten.

Dadurch dass die Klasse \texttt{Portal} von der Klasse \texttt{GroupNode} erbt,
übernimmt sie alle Eigenschaften eines Szeneknoten. Das bedeutet eine
Unterklasse der \texttt{Portal}-Klasse lässt sich ohne Probleme transformieren
und in einer Szene platzieren.

\subsection{Teleporter-Klasse}



\subsubsection{PortalConnector}

\subsubsection{Physik}

\subsection{Mirror-Klasse}

\subsection{Door-Klasse}

\subsection{PortalList}

\subsection{Beispiel einer Portalanwendung}

\begin{lstlisting}[captionpos=b,caption={Struktur einer Portalanwendung},label=lst:exa] 
public static void main(String[] args){ 
	// create root node
	GroupNode root = new GroupNode();
	// some scene nodes added ...
	// camera and light node added ...
	CameraNode cam = new CameraNode("cam", 4/3f, 60);
	
	// create Pipeline
	Pipeline p = new Pipeline();
	
	// create teleporter
	// needs pipeline for further rendering
	Teleporter portal1 = new Teleporter(p, "portal1");
	
	// create another teleporter
	Teleporter portal2 = new Teleporter(p, "portal2");
	
	// connect both teleporter
	PortalConnector.connect(portal1, portal2);
	
	// some pipelinecommands ...
	
	// let the list render the scene into the portals
	PortalList.render();
	
	// create render window
	RenderWindow win = new NewtRenderWindow(p);
	
	// create viewer
	Viewer v = new Viewer(win)
	
	// render loop
	while(v.isRunning())
	{
		long start = System.currentTimeMillis();
		
		// render next frame
		v.display();
		
		// move speed of the camera
		double moveSpeed = (
			System.currentTimeMillis()	
			- start) * 0.005f;
		
		// upate scene rendered in the portals
		PortalList.update(camera, moveSpeed);
	}
}
\end{lstlisting}

\section{Portal Culling}

\subsection{Cell-Klasse}

\subsubsection{CellList}

\subsubsection{Wall-Klasse}

\section{Optimierungen}

\chapter{Ergebnisse und Bewertung}

\chapter{Zusammenfassung und Ausblick}

\bibliography{lit}
\bibliographystyle{apalike}
\addcontentsline{toc}{chapter}{Literaturverzeichnis}

\end{document}